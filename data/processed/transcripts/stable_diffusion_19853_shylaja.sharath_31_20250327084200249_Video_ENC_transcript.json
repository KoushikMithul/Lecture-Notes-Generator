{
  "video_name": "stable_diffusion_19853_shylaja.sharath_31_20250327084200249_Video_ENC",
  "video_path": "data/raw/videos/stable_diffusion_19853_shylaja.sharath_31_20250327084200249_Video_ENC.mp4",
  "duration_seconds": 3299.968,
  "transcript": [
    {
      "text": " .",
      "start_time": 0.0,
      "end_time": 2.0
    },
    {
      "text": " .",
      "start_time": 30.0,
      "end_time": 32.0
    },
    {
      "text": " .",
      "start_time": 60.0,
      "end_time": 62.0
    },
    {
      "text": " .",
      "start_time": 90.0,
      "end_time": 92.0
    },
    {
      "text": " .",
      "start_time": 120.0,
      "end_time": 122.0
    },
    {
      "text": " .",
      "start_time": 150.0,
      "end_time": 152.0
    },
    {
      "text": " .",
      "start_time": 180.0,
      "end_time": 182.0
    },
    {
      "text": " .",
      "start_time": 210.0,
      "end_time": 212.0
    },
    {
      "text": " .",
      "start_time": 240.0,
      "end_time": 242.0
    },
    {
      "text": " .",
      "start_time": 270.0,
      "end_time": 272.0
    },
    {
      "text": " .",
      "start_time": 300.0,
      "end_time": 302.0
    },
    {
      "text": " Thank you.",
      "start_time": 330.0,
      "end_time": 359.0
    },
    {
      "text": " I'm sorry.",
      "start_time": 360.0,
      "end_time": 362.0
    },
    {
      "text": " The",
      "start_time": 390.0,
      "end_time": 419.8
    },
    {
      "text": "",
      "start_time": 419.8,
      "end_time": 390.0
    },
    {
      "text": " you",
      "start_time": 392.06,
      "end_time": null
    },
    {
      "text": " The",
      "start_time": 420.0,
      "end_time": 422.0
    },
    {
      "text": " Good morning. Shall I start? In fact, last three days I was in CEDAC, which is a government",
      "start_time": 450.0,
      "end_time": 462.76
    },
    {
      "text": " organization and I got to hear a lot of technology horizons and one day was on AI and LLM and",
      "start_time": 462.76,
      "end_time": 475.92
    },
    {
      "text": "",
      "start_time": 475.92,
      "end_time": 450.0
    },
    {
      "text": " I felt very happy that we are on par with what they are working.",
      "start_time": 453.88,
      "end_time": null
    },
    {
      "text": " So, I did in fact, it is not about talking about myself, I just want to say when there",
      "start_time": 480.0,
      "end_time": 488.08
    },
    {
      "text": " were some questions asked by C. Rack, I was able to answer, when the speaker did not answer,",
      "start_time": 488.08,
      "end_time": 493.2
    },
    {
      "text": " but I answered it.",
      "start_time": 493.2,
      "end_time": 494.2
    },
    {
      "text": " So, that happened, so that gave us lot of satisfaction and in fact, they all came back",
      "start_time": 494.2,
      "end_time": 498.96
    },
    {
      "text": " to me, reached out, said that they want to collaborate and all that because we started",
      "start_time": 498.96,
      "end_time": 503.12
    },
    {
      "text": " answering these questions.",
      "start_time": 503.12,
      "end_time": 504.62
    },
    {
      "text": "",
      "start_time": 504.62,
      "end_time": 480.0
    },
    {
      "text": " So that was the success and they want to come back and conduct hackathons and",
      "start_time": 485.2,
      "end_time": null
    },
    {
      "text": " provide internships and all of this. So I have had some success in meeting them.",
      "start_time": 510.0,
      "end_time": 514.76
    },
    {
      "text": " Hopefully we will get to know more about it and if I can facilitate some internships",
      "start_time": 514.76,
      "end_time": 519.24
    },
    {
      "text": " it would be good. So this is the success but apart from this what I felt is we are doing",
      "start_time": 519.24,
      "end_time": 524.76
    },
    {
      "text": " everything in silos where we lack here is that we teach LLM separately, high performance",
      "start_time": 524.76,
      "end_time": 530.16
    },
    {
      "text": " clusters separately. There is no connectivity between the bottom layer like the Q-Lora thing",
      "start_time": 530.16,
      "end_time": 535.84
    },
    {
      "text": "",
      "start_time": 535.84,
      "end_time": 510.0
    },
    {
      "text": " half precision full precision and all of this okay we do not connect so much.",
      "start_time": 514.08,
      "end_time": null
    },
    {
      "text": " So all the topics are conducted in silos, so we really do not get to know the entire",
      "start_time": 540.0,
      "end_time": 546.28
    },
    {
      "text": " stack of layers in a single project or a single course but that is anyway up to you to connect",
      "start_time": 546.28,
      "end_time": 551.96
    },
    {
      "text": " all the different pieces.",
      "start_time": 551.96,
      "end_time": 553.46
    },
    {
      "text": " So lot of very good work is happening and there is lot of emphasis given on supercomputers",
      "start_time": 553.46,
      "end_time": 559.7
    },
    {
      "text": " and developing processes, ohm processor is what they are developing, a processor from",
      "start_time": 559.7,
      "end_time": 564.6
    },
    {
      "text": "",
      "start_time": 564.6,
      "end_time": 540.0
    },
    {
      "text": " India and they want to develop LLM from India also like this on India.",
      "start_time": 545.44,
      "end_time": null
    },
    {
      "text": " We are already developing one on Sanskrit.",
      "start_time": 570.0,
      "end_time": 573.8
    },
    {
      "text": " So that work was happening on mental health they are working on.",
      "start_time": 573.8,
      "end_time": 576.8
    },
    {
      "text": " So we are working on that project as well.",
      "start_time": 576.8,
      "end_time": 579.08
    },
    {
      "text": " So I mean there is whatever projects we have given here.",
      "start_time": 579.08,
      "end_time": 583.6
    },
    {
      "text": " In fact similar work is happening over them.",
      "start_time": 583.6,
      "end_time": 586.12
    },
    {
      "text": " So that gives us lot of satisfaction and peace that we are doing correct here.",
      "start_time": 586.12,
      "end_time": 591.36
    },
    {
      "text": " So that is what happened.",
      "start_time": 591.36,
      "end_time": 592.6
    },
    {
      "text": " But only thing is they have access to very high end computing and there is funding there",
      "start_time": 592.6,
      "end_time": 598.0
    },
    {
      "text": "",
      "start_time": 598.0,
      "end_time": 570.0
    },
    {
      "text": " no issue at all for them.",
      "start_time": 571.72,
      "end_time": null
    },
    {
      "text": " So and the data set they are in collaboration with Neman's.",
      "start_time": 600.0,
      "end_time": 604.28
    },
    {
      "text": " So Neman's will provide a lot of data and they are in collaboration with the India's",
      "start_time": 604.28,
      "end_time": 608.88
    },
    {
      "text": " all the repositories.",
      "start_time": 608.88,
      "end_time": 609.88
    },
    {
      "text": " So digitization everything they are doing it but right now one pain point is that they",
      "start_time": 609.88,
      "end_time": 614.76
    },
    {
      "text": " cannot share those resources to us.",
      "start_time": 614.76,
      "end_time": 617.88
    },
    {
      "text": " So if in the absence of that how do we get hold of that so that is what we were discussing",
      "start_time": 617.88,
      "end_time": 622.52
    },
    {
      "text": " so let us see how we proceed.",
      "start_time": 622.52,
      "end_time": 624.32
    },
    {
      "text": " So this is in gist of whatever happened in the last three days and quantum computing",
      "start_time": 624.32,
      "end_time": 629.08
    },
    {
      "text": "",
      "start_time": 629.08,
      "end_time": 600.0
    },
    {
      "text": " is also going in.",
      "start_time": 600.92,
      "end_time": null
    },
    {
      "text": " a big way that is what I saw that lot of emphasis is given, semiconductor design there is lot",
      "start_time": 630.0,
      "end_time": 635.74
    },
    {
      "text": " of emphasis given.",
      "start_time": 635.74,
      "end_time": 636.74
    },
    {
      "text": " So I am not sure in which field you would like to and one of the IAC person also spoke",
      "start_time": 636.74,
      "end_time": 641.94
    },
    {
      "text": " about some neural computing related, neural I do not remember the exact name.",
      "start_time": 641.94,
      "end_time": 646.82
    },
    {
      "text": " So there is one he actually did it damn good okay from last 10 years and that is the way",
      "start_time": 646.82,
      "end_time": 652.94
    },
    {
      "text": " to compute he says.",
      "start_time": 652.94,
      "end_time": 655.1
    },
    {
      "text": "",
      "start_time": 655.1,
      "end_time": 630.0
    },
    {
      "text": " So there are different computing dimensions we need to look at developing LLM on.",
      "start_time": 634.92,
      "end_time": null
    },
    {
      "text": " So, that marriage of computing and LLM that we have not discussed.",
      "start_time": 660.0,
      "end_time": 666.24
    },
    {
      "text": " So, that is the gap that we have in the classes.",
      "start_time": 666.24,
      "end_time": 669.12
    },
    {
      "text": " So, this is my understanding I thought I will just share some information with you on that.",
      "start_time": 669.12,
      "end_time": 674.56
    },
    {
      "text": " So, with that now we will start.",
      "start_time": 674.56,
      "end_time": 676.76
    },
    {
      "text": " So, so far we have looked at in the course in the last 3 units we have looked at how",
      "start_time": 676.76,
      "end_time": 684.12
    },
    {
      "text": "",
      "start_time": 684.12,
      "end_time": 660.0
    },
    {
      "text": " we are able to generate the text in a better way by incorporating techniques like prompt",
      "start_time": 665.84,
      "end_time": 660.0
    },
    {
      "text": " Thank you.",
      "start_time": 661.0,
      "end_time": null
    },
    {
      "text": " engineering, NENRAC techniques and all of that, so that the response from the LLM would",
      "start_time": 690.0,
      "end_time": 695.48
    },
    {
      "text": " be more.",
      "start_time": 695.48,
      "end_time": 696.48
    },
    {
      "text": " But primarily our focus was on text generation rather than any other multimodal applications.",
      "start_time": 696.48,
      "end_time": 702.04
    },
    {
      "text": " So from today onwards, this is the fourth unit.",
      "start_time": 702.04,
      "end_time": 705.48
    },
    {
      "text": " So our emphasis is on multimodal plus there is another important concept in LLM that you",
      "start_time": 705.48,
      "end_time": 711.72
    },
    {
      "text": " need to understand which is the agentic approach.",
      "start_time": 711.72,
      "end_time": 714.68
    },
    {
      "text": " So these are the two main highlights that we need to talk about.",
      "start_time": 714.68,
      "end_time": 717.68
    },
    {
      "text": " So today we will discuss on that.",
      "start_time": 717.68,
      "end_time": 719.28
    },
    {
      "text": "",
      "start_time": 719.28,
      "end_time": 690.0
    },
    {
      "text": " time",
      "start_time": 692.0,
      "end_time": null
    },
    {
      "text": " to first speak about something called as a image generation. Image generation although",
      "start_time": 720.0,
      "end_time": 725.28
    },
    {
      "text": " is not exactly an LLM like text in it is similar in LLMs in the sense that LLMs are good at",
      "start_time": 725.28,
      "end_time": 732.8
    },
    {
      "text": " generating text and there are generation engines which are good at images. In that sense image",
      "start_time": 732.8,
      "end_time": 739.4
    },
    {
      "text": " generation can also be called similar to that of LLM but it is not exactly LLM because",
      "start_time": 739.4,
      "end_time": 745.08
    },
    {
      "text": " it is not a large language kind of model.",
      "start_time": 745.08,
      "end_time": 747.36
    },
    {
      "text": "",
      "start_time": 747.36,
      "end_time": 720.0
    },
    {
      "text": " So, those are the techniques that we need to look at.",
      "start_time": 722.64,
      "end_time": null
    },
    {
      "text": " and then we will proceed to multimodal LLM. So, today's main agenda is about image generation.",
      "start_time": 750.0,
      "end_time": 757.0
    },
    {
      "text": " So, I think in your other TDL applications you must have already seen this concept. Have",
      "start_time": 757.0,
      "end_time": 773.36
    },
    {
      "text": "",
      "start_time": 773.36,
      "end_time": 750.0
    },
    {
      "text": " you looked into any of the algorithms which incorporates image generation in the",
      "start_time": 757.0,
      "end_time": null
    },
    {
      "text": " topics in deep learning course, GAN, VAE, GAN and so on right. So, these are the techniques",
      "start_time": 780.0,
      "end_time": 793.12
    },
    {
      "text": " that have been adopted in deep learning to generate images. I will speak about VAE little",
      "start_time": 793.12,
      "end_time": 800.76
    },
    {
      "text": " later, first let us look at how the GAN operates, Generative Adversarial Network. The GAN is",
      "start_time": 800.76,
      "end_time": 808.8
    },
    {
      "text": "",
      "start_time": 808.8,
      "end_time": 780.0
    },
    {
      "text": " do that generate.",
      "start_time": 781.24,
      "end_time": null
    },
    {
      "text": " So, how does it create these images?",
      "start_time": 810.0,
      "end_time": 815.52
    },
    {
      "text": " What are the two main pieces of GAN if you are aware of it?",
      "start_time": 815.52,
      "end_time": 820.92
    },
    {
      "text": " It has a discriminator and a generator.",
      "start_time": 820.92,
      "end_time": 823.24
    },
    {
      "text": " So, there is a discriminator and there is a generator and to the discriminator you are",
      "start_time": 823.24,
      "end_time": 829.52
    },
    {
      "text": " going to give from real images, real images and then generator is going to generate fake",
      "start_time": 829.52,
      "end_time": 837.04
    },
    {
      "text": "",
      "start_time": 837.04,
      "end_time": 810.0
    },
    {
      "text": " images and discriminators job.",
      "start_time": 812.96,
      "end_time": null
    },
    {
      "text": " job is to distinguish that the image given by the generator is fake, it has to discriminate",
      "start_time": 840.0,
      "end_time": 846.2
    },
    {
      "text": " between these two. So, the training of the discriminator is done keeping the generator",
      "start_time": 846.2,
      "end_time": 851.08
    },
    {
      "text": " constant, generator is fixed during training time generator is fixed it generates fake",
      "start_time": 851.08,
      "end_time": 856.32
    },
    {
      "text": " image and this fake image distribution should be as close to the distribution of the real",
      "start_time": 856.32,
      "end_time": 862.2
    },
    {
      "text": " image. So, that is the job of the generator, the discriminator job is to distinguish between",
      "start_time": 862.2,
      "end_time": 868.64
    },
    {
      "text": "",
      "start_time": 868.64,
      "end_time": 840.0
    },
    {
      "text": " the two. So, they do.",
      "start_time": 841.34,
      "end_time": null
    },
    {
      "text": " So, in like devil's advocate kind of scenario, so first discriminator is trained to make",
      "start_time": 870.0,
      "end_time": 876.44
    },
    {
      "text": " sure that it discriminates, but generators job is then discriminator switch and then",
      "start_time": 876.44,
      "end_time": 881.08
    },
    {
      "text": " generator is trained. Generator job is to produce an image which is as close to real",
      "start_time": 881.08,
      "end_time": 887.68
    },
    {
      "text": " as possible that is the objective of the generator function.",
      "start_time": 887.68,
      "end_time": 892.12
    },
    {
      "text": " Now, so these basically we are comparing the distributions of the images produced by the",
      "start_time": 892.12,
      "end_time": 897.56
    },
    {
      "text": "",
      "start_time": 897.56,
      "end_time": 870.0
    },
    {
      "text": " the generator and the, so, the generator is the generator.",
      "start_time": 872.44,
      "end_time": null
    },
    {
      "text": " The real image and the discriminator's output is compared, the distributions are compared",
      "start_time": 900.0,
      "end_time": 907.12
    },
    {
      "text": " and when we compare the distributions, how are the two distributions compared?",
      "start_time": 907.12,
      "end_time": 912.04
    },
    {
      "text": " We either use KL divergence, full back libner divergence function for comparing the two",
      "start_time": 912.04,
      "end_time": 917.64
    },
    {
      "text": " distributions or Jensen-Shanen divergence or Wasserstein-Gern you must have heard about.",
      "start_time": 917.64,
      "end_time": 923.44
    },
    {
      "text": " So Wasserstein's distance is also used.",
      "start_time": 923.44,
      "end_time": 926.0
    },
    {
      "text": "",
      "start_time": 926.0,
      "end_time": 900.0
    },
    {
      "text": " So these are the methodologies, they are different from your regular laws.",
      "start_time": 904.0,
      "end_time": null
    },
    {
      "text": " function. If you want to compare distribution versus distribution, one distribution is like",
      "start_time": 930.0,
      "end_time": 934.68
    },
    {
      "text": " this, another distribution is like this. If you want to compare these two distributions,",
      "start_time": 934.68,
      "end_time": 938.88
    },
    {
      "text": " this is fake image, this is real image. If you want to compare these two distributions,",
      "start_time": 938.88,
      "end_time": 944.2
    },
    {
      "text": " you have these specialized loss functions called as Kulbach-Libner, Jensen's Shannon",
      "start_time": 944.2,
      "end_time": 949.68
    },
    {
      "text": " divergence and versus strain distance. They compare distributions. So the overall area",
      "start_time": 949.68,
      "end_time": 956.28
    },
    {
      "text": "",
      "start_time": 956.28,
      "end_time": 930.0
    },
    {
      "text": " is compared it is split and all of that. So, from internal operation.",
      "start_time": 933.72,
      "end_time": null
    },
    {
      "text": " are done that is not the topic of discussion now in detail but you will have to study this.",
      "start_time": 960.0,
      "end_time": 965.68
    },
    {
      "text": " So these losses are going to be reduced in the case of VAE autoencoder, GAN and all of",
      "start_time": 965.68,
      "end_time": 971.72
    },
    {
      "text": " this okay. So just to see that whether the distribution is closed or not. So like that",
      "start_time": 971.72,
      "end_time": 976.52
    },
    {
      "text": " the generator produces but how does the generator produce this image? How does it generate the",
      "start_time": 976.52,
      "end_time": 982.88
    },
    {
      "text": " image? It generates that image from a noisy image which is from a Gaussian distribution.",
      "start_time": 982.88,
      "end_time": 988.64
    },
    {
      "text": "",
      "start_time": 988.64,
      "end_time": 960.0
    },
    {
      "text": " samples for Gaussian.",
      "start_time": 961.34,
      "end_time": null
    },
    {
      "text": " distribution and that is a sampled one is a noisy image. So, you are going to sample",
      "start_time": 990.0,
      "end_time": 996.04
    },
    {
      "text": " a noisy image and you are going to produce that of a cat or something man face of a person.",
      "start_time": 996.04,
      "end_time": 1002.92
    },
    {
      "text": " So, the generator's job here in the GAN case the generator's job is very drastic in the",
      "start_time": 1002.92,
      "end_time": 1009.48
    },
    {
      "text": " sense from one noise image directly you will produce the image that is the job of the generator.",
      "start_time": 1009.48,
      "end_time": 1015.4
    },
    {
      "text": "",
      "start_time": 1015.4,
      "end_time": 990.0
    },
    {
      "text": " This is again a CNN, this is again a CNN. Both are deep networks.",
      "start_time": 992.52,
      "end_time": 994.6
    },
    {
      "text": " a deep network, this is a deep network and from this noisy image you are going to produce",
      "start_time": 1020.0,
      "end_time": 1024.64
    },
    {
      "text": " this image, it is just one step process, noise converted into an image, but the drift should",
      "start_time": 1024.64,
      "end_time": 1031.88
    },
    {
      "text": " be towards real image. If real image consists of all face images and this produces all car",
      "start_time": 1031.88,
      "end_time": 1037.72
    },
    {
      "text": " images, then it is there is a total drift in the probability distribution. So, then this",
      "start_time": 1037.72,
      "end_time": 1043.44
    },
    {
      "text": " divergence is too much, so the divergence you try to minimize. So, that is how the KL",
      "start_time": 1043.44,
      "end_time": 1048.6
    },
    {
      "text": "",
      "start_time": 1048.6,
      "end_time": 1020.0
    },
    {
      "text": " divergence is going to be used in.",
      "start_time": 1021.4,
      "end_time": null
    },
    {
      "text": " the GaNS equations there will load whole lot of mathematics behind that but anyway if you",
      "start_time": 1050.0,
      "end_time": 1054.36
    },
    {
      "text": " understand the idea behind what GaNS does it is good enough for our discussion now.",
      "start_time": 1054.36,
      "end_time": 1059.56
    },
    {
      "text": " So the generator the way the GaNS operate in creating a image is very drastic from very",
      "start_time": 1059.56,
      "end_time": 1066.0
    },
    {
      "text": " noisy image generally you expected to produce a distribution which is as close to real okay",
      "start_time": 1066.0,
      "end_time": 1072.24
    },
    {
      "text": " very drastic step. So that in that sense the GaNS were not really very good at producing",
      "start_time": 1072.24,
      "end_time": 1079.04
    },
    {
      "text": "",
      "start_time": 1079.04,
      "end_time": 1050.0
    },
    {
      "text": " high quality images.",
      "start_time": 1051.0,
      "end_time": null
    },
    {
      "text": " images. GANS suffers from producing high quality images. So this was the issue with the GANS.",
      "start_time": 1080.0,
      "end_time": 1086.56
    },
    {
      "text": " So that is when they thought of using a different methodology altogether to generate or produce",
      "start_time": 1086.56,
      "end_time": 1092.76
    },
    {
      "text": " high quality images. So these are called as diffusion models which is used in stable diffusion.",
      "start_time": 1092.76,
      "end_time": 1098.08
    },
    {
      "text": "",
      "start_time": 1098.08,
      "end_time": 1080.0
    },
    {
      "text": " So that is the topic of our discussion now.",
      "start_time": 1081.76,
      "end_time": null
    },
    {
      "text": " We will have to study diffusion models.",
      "start_time": 1110.0,
      "end_time": 1120.84
    },
    {
      "text": " You have to concentrate on the class, close the laptop, diffusion models.",
      "start_time": 1120.84,
      "end_time": 1129.96
    },
    {
      "text": "",
      "start_time": 1129.96,
      "end_time": 1110.0
    },
    {
      "text": " So, in the diffusion models, unlike GANS, we do a step by step process.",
      "start_time": 1119.4,
      "end_time": null
    },
    {
      "text": " There are two processes followed, one is a forward process and the second one is a reverse",
      "start_time": 1140.0,
      "end_time": 1149.56
    },
    {
      "text": " process.",
      "start_time": 1149.56,
      "end_time": 1155.24
    },
    {
      "text": " Forward process is used for some training, reverse process is actually used for generation.",
      "start_time": 1155.24,
      "end_time": 1163.64
    },
    {
      "text": " Forward process is used in training the stable diffusion models and reverse process is used",
      "start_time": 1163.64,
      "end_time": 1169.08
    },
    {
      "text": "",
      "start_time": 1169.08,
      "end_time": 1140.0
    },
    {
      "text": " at the time of.",
      "start_time": 1140.64,
      "end_time": null
    },
    {
      "text": " inferencing the sense here the objective is to generate images. So what the forward process",
      "start_time": 1170.0,
      "end_time": 1175.44
    },
    {
      "text": " does is you are given an image training image. So training must have proper images. So training",
      "start_time": 1175.44,
      "end_time": 1183.44
    },
    {
      "text": " set is consisting of proper images of that of a person or car, cat, dog whatever. So",
      "start_time": 1183.44,
      "end_time": 1189.48
    },
    {
      "text": " this is a proper image. The in the forward process what you are going to do is to add",
      "start_time": 1189.48,
      "end_time": 1196.76
    },
    {
      "text": "",
      "start_time": 1196.76,
      "end_time": 1170.0
    },
    {
      "text": " some noise add some noise",
      "start_time": 1175.0,
      "end_time": null
    },
    {
      "text": " into this. So and that noise is going to be scheduled. How much noise you have to add",
      "start_time": 1200.0,
      "end_time": 1205.9
    },
    {
      "text": " is characterized by a term called beta t, beta t. Beta t talks about the noise schedule,",
      "start_time": 1205.9,
      "end_time": 1212.62
    },
    {
      "text": " how much to be added at time stamp t that is called as beta t, noise control parameter.",
      "start_time": 1212.62,
      "end_time": 1219.38
    },
    {
      "text": " So beta t will get added to this and you will be producing another image but with some noise",
      "start_time": 1219.38,
      "end_time": 1227.7
    },
    {
      "text": "",
      "start_time": 1227.7,
      "end_time": 1200.0
    },
    {
      "text": " added some nice added",
      "start_time": 1204.16,
      "end_time": null
    },
    {
      "text": " then this is a time step this is a t 0 let us say this is a time step t 1 again at time",
      "start_time": 1230.0,
      "end_time": 1242.84
    },
    {
      "text": "",
      "start_time": 1242.84,
      "end_time": 1230.0
    },
    {
      "text": " timestamp t2, further noise is added already this is getting blurred ok. So, like this",
      "start_time": 1242.56,
      "end_time": 1230.0
    },
    {
      "text": " if you are",
      "start_time": 1237.0,
      "end_time": null
    },
    {
      "text": " Yeah, beta is derived from the sample from a normal distribution. Yeah, every time the",
      "start_time": 1260.0,
      "end_time": 1269.56
    },
    {
      "text": " noise has to be sampled from a normal distribution and the noise is injected into the image.",
      "start_time": 1269.56,
      "end_time": 1281.0
    },
    {
      "text": " It is not the same amount, every time it is a different amount of noise which is drawn",
      "start_time": 1281.0,
      "end_time": 1285.92
    },
    {
      "text": "",
      "start_time": 1285.92,
      "end_time": 1260.0
    },
    {
      "text": " from a normal distribution.",
      "start_time": 1262.32,
      "end_time": null
    },
    {
      "text": " like this, like that. This is the forward diffusion process, ok.",
      "start_time": 1290.0,
      "end_time": 1319.56
    },
    {
      "text": "",
      "start_time": 1319.56,
      "end_time": 1290.0
    },
    {
      "text": " the problem.",
      "start_time": 1290.5,
      "end_time": null
    },
    {
      "text": " probability of generating x t from x t minus 1 probability see this is a probability what",
      "start_time": 1320.0,
      "end_time": 1328.44
    },
    {
      "text": " kind of image you will generate from this is a probabilistic number because you are",
      "start_time": 1328.44,
      "end_time": 1333.2
    },
    {
      "text": " inducing any sample can be drawn. So, it is a probability probability of generating x",
      "start_time": 1333.2,
      "end_time": 1337.84
    },
    {
      "text": " t given x t minus 1 is equal to from a normal distribution you are going to generate x t",
      "start_time": 1337.84,
      "end_time": 1344.56
    },
    {
      "text": "",
      "start_time": 1344.56,
      "end_time": 1320.0
    },
    {
      "text": " can call it as x t, x t.",
      "start_time": 1327.0,
      "end_time": null
    },
    {
      "text": " 1 minus 1 minus beta t root xt minus 1 beta t i. This is the mean, this is the variance.",
      "start_time": 1350.0,
      "end_time": 1367.76
    },
    {
      "text": " This is drawn from a normal distribution with this mean and this variance you are going",
      "start_time": 1367.76,
      "end_time": 1374.96
    },
    {
      "text": "",
      "start_time": 1374.96,
      "end_time": 1350.0
    },
    {
      "text": " to generate xt that is the meaning of this equation forward diffusion process probability of the diffusion process.",
      "start_time": 1355.0,
      "end_time": 1355.04
    },
    {
      "text": " of generating x t from x t minus 1 is based on deriving the sample from a normal distribution",
      "start_time": 1380.0,
      "end_time": 1386.96
    },
    {
      "text": " with mean mu and sigma variance and deriving the image x t that is the meaning of this",
      "start_time": 1386.96,
      "end_time": 1394.24
    },
    {
      "text": " equation this is the forward diffusion equation. So, every time you are going to draw the noise",
      "start_time": 1394.24,
      "end_time": 1401.0
    },
    {
      "text": " set the beta parameter and draw the noise from a normal distribution and then you are",
      "start_time": 1401.0,
      "end_time": 1405.0
    },
    {
      "text": "",
      "start_time": 1405.0,
      "end_time": 1380.0
    },
    {
      "text": " going to inject this noise at successive samples. So, this is known as a noise schedule.",
      "start_time": 1387.0,
      "end_time": null
    },
    {
      "text": " The noise schedule has been set to T's time stamps, T plus 1 basically, but T 0 is not",
      "start_time": 1410.0,
      "end_time": 1417.44
    },
    {
      "text": " a noisy image, but rest all the images are noisy images. T 1 to x t, okay, T represents",
      "start_time": 1417.44,
      "end_time": 1427.28
    },
    {
      "text": " the time stamp and x represents the image, x t minus 1 is the image that is produced",
      "start_time": 1427.28,
      "end_time": 1433.24
    },
    {
      "text": " at time stamp T minus 1 and this is produced at time t. Finally, a complete noisy image",
      "start_time": 1433.24,
      "end_time": 1439.2
    },
    {
      "text": "",
      "start_time": 1439.2,
      "end_time": 1410.0
    },
    {
      "text": " produced.",
      "start_time": 1412.44,
      "end_time": null
    },
    {
      "text": " Exactly the reverse process is in reverse diffusion process.",
      "start_time": 1440.0,
      "end_time": 1444.8
    },
    {
      "text": " So, you are going to start from Xd a noisy image that is what the GAN did.",
      "start_time": 1444.8,
      "end_time": 1451.0
    },
    {
      "text": " The GAN's approach is like this, you have a noisy image Xd and GAN what it did do?",
      "start_time": 1451.0,
      "end_time": 1457.08
    },
    {
      "text": " It directly produced with this image.",
      "start_time": 1457.08,
      "end_time": 1460.08
    },
    {
      "text": " The GAN produced that noisy image directly into this that is where the stable diffusion",
      "start_time": 1460.08,
      "end_time": 1465.68
    },
    {
      "text": " differs or the diffusion models differ.",
      "start_time": 1465.68,
      "end_time": 1468.0
    },
    {
      "text": "",
      "start_time": 1468.0,
      "end_time": 1440.0
    },
    {
      "text": " Instead of producing from that noise,",
      "start_time": 1442.0,
      "end_time": null
    },
    {
      "text": " see image to this image which is very drastic. So, it is really difficult to learn such a",
      "start_time": 1470.0,
      "end_time": 1475.2
    },
    {
      "text": " process what it does is it produces this image first. So, in the reverse diffusion process",
      "start_time": 1475.2,
      "end_time": 1482.04
    },
    {
      "text": "",
      "start_time": 1482.04,
      "end_time": 1470.0
    },
    {
      "text": " q of x t minus 1 is generated from x t is again using another normal distribution.",
      "start_time": 1477.64,
      "end_time": null
    },
    {
      "text": " So, this is the reverse distribution process. It is a probability of generating this image,",
      "start_time": 1500.0,
      "end_time": 1513.08
    },
    {
      "text": " given this image, Q is the probability of generating Xt minus 1 given Xt from a normal",
      "start_time": 1513.08,
      "end_time": 1518.92
    },
    {
      "text": " distribution again, you generate Xt minus 1 taking mu and sigma and then you are going",
      "start_time": 1518.92,
      "end_time": 1526.44
    },
    {
      "text": "",
      "start_time": 1526.44,
      "end_time": 1500.0
    },
    {
      "text": " to formulate that.",
      "start_time": 1501.08,
      "end_time": null
    },
    {
      "text": " is basically the process but how is it done in implementation that we will see later but",
      "start_time": 1530.0,
      "end_time": 1535.32
    },
    {
      "text": " this is what it does okay.",
      "start_time": 1535.32,
      "end_time": 1539.56
    },
    {
      "text": " So once the noise is characterized in the reverse diffusion process you are going to",
      "start_time": 1539.56,
      "end_time": 1544.0
    },
    {
      "text": " subtract that noise and you are going to get this and subtract the noise and get it and",
      "start_time": 1544.0,
      "end_time": 1548.08
    },
    {
      "text": " so on.",
      "start_time": 1548.08,
      "end_time": 1549.08
    },
    {
      "text": " So normal distribution is going to fetch the noise for you okay and that noise has to be",
      "start_time": 1549.08,
      "end_time": 1553.2
    },
    {
      "text": " subtracted and you are going to get a better image and it will drift towards the kind of",
      "start_time": 1553.2,
      "end_time": 1558.48
    },
    {
      "text": "",
      "start_time": 1558.48,
      "end_time": 1530.0
    },
    {
      "text": " the image that you want.",
      "start_time": 1530.92,
      "end_time": null
    },
    {
      "text": " So, in successive steps, so given xt you are not going to generate x0 directly, given xt",
      "start_time": 1560.0,
      "end_time": 1568.28
    },
    {
      "text": " you are going to generate xt-1, given xt-1 you are going to generate xt-2, xt-2 will",
      "start_time": 1568.28,
      "end_time": 1574.0
    },
    {
      "text": " produce xt-3.",
      "start_time": 1574.0,
      "end_time": 1575.0
    },
    {
      "text": " So, small amount of noise is reduced, okay, how much noise to be reduced from a given",
      "start_time": 1575.0,
      "end_time": 1580.32
    },
    {
      "text": " image that has to be seen, but it does that.",
      "start_time": 1580.32,
      "end_time": 1583.28
    },
    {
      "text": " In the reverse process basically it is about subtracting the noise in the image so that",
      "start_time": 1583.28,
      "end_time": 1588.4
    },
    {
      "text": "",
      "start_time": 1588.4,
      "end_time": 1560.0
    },
    {
      "text": " you get to that ship.",
      "start_time": 1561.34,
      "end_time": null
    },
    {
      "text": " But how much noise and where that noise has to be reduced has to be learnt.",
      "start_time": 1590.0,
      "end_time": 1593.72
    },
    {
      "text": " It is not like simply removing noise.",
      "start_time": 1593.72,
      "end_time": 1596.32
    },
    {
      "text": " You have to get to drift it towards a person.",
      "start_time": 1596.32,
      "end_time": 1598.76
    },
    {
      "text": " You have to drift it towards a car.",
      "start_time": 1598.76,
      "end_time": 1600.4
    },
    {
      "text": " You have to get that image right.",
      "start_time": 1600.4,
      "end_time": 1602.04
    },
    {
      "text": " So how is that drifting done is what is done in the stable diffusion.",
      "start_time": 1602.04,
      "end_time": 1606.4
    },
    {
      "text": " But generally diffusion models work based on this principle of adding noise in the forward",
      "start_time": 1606.4,
      "end_time": 1611.4
    },
    {
      "text": " process and removing noise in the reverse process.",
      "start_time": 1611.4,
      "end_time": 1615.16
    },
    {
      "text": "",
      "start_time": 1615.16,
      "end_time": 1590.0
    },
    {
      "text": " diffusion process as successfully noise to successive images.",
      "start_time": 1594.8,
      "end_time": null
    },
    {
      "text": " and moves towards pure noise image and in the reverse diffusion process from the pure",
      "start_time": 1620.0,
      "end_time": 1626.48
    },
    {
      "text": " noise you start removing noise at random places and drift towards generating an image which",
      "start_time": 1626.48,
      "end_time": 1633.44
    },
    {
      "text": " you want. So that is the reverse diffusion process. Both of them are drawn from a normal",
      "start_time": 1633.44,
      "end_time": 1638.0
    },
    {
      "text": " distribution the noise related aspects. So this is forward and reverse diffusion.",
      "start_time": 1638.0,
      "end_time": 1644.48
    },
    {
      "text": "",
      "start_time": 1644.48,
      "end_time": 1620.0
    },
    {
      "text": " Now if you look at this image let us say this is some 100 percent.",
      "start_time": 1627.0,
      "end_time": null
    },
    {
      "text": " 500 cross 100 or 500 cross 500 resolution image. So this 500 cross 500 you have to do",
      "start_time": 1650.0,
      "end_time": 1660.6
    },
    {
      "text": " it for T times times. So they felt that computationally this is very very expensive adding noise",
      "start_time": 1660.6,
      "end_time": 1668.44
    },
    {
      "text": " every time to a 500 cross 500 pixel which is like 25, 250,000. 250,000 set of values",
      "start_time": 1668.44,
      "end_time": 1677.84
    },
    {
      "text": "",
      "start_time": 1677.84,
      "end_time": 1650.0
    },
    {
      "text": " will have to be perturbed.",
      "start_time": 1652.16,
      "end_time": null
    },
    {
      "text": " will have to be perturbed every time by adding noise. So, 250,000 here, 250,000 here, 250,000",
      "start_time": 1680.0,
      "end_time": 1687.0
    },
    {
      "text": " here, it is still a very small resolution. So, high resolution 2000 and all that will",
      "start_time": 1687.0,
      "end_time": 1693.32
    },
    {
      "text": " be very, very expensive. So, what they decided is instead of the diffusion",
      "start_time": 1693.32,
      "end_time": 1697.88
    },
    {
      "text": " model they went ahead something with a concept called as latent diffusion model. So, instead",
      "start_time": 1697.88,
      "end_time": 1707.96
    },
    {
      "text": "",
      "start_time": 1707.96,
      "end_time": 1680.0
    },
    {
      "text": " of operating in the image space.",
      "start_time": 1682.0,
      "end_time": null
    },
    {
      "text": " the diffusion models operate in the image space whereas the latent diffusion models",
      "start_time": 1710.0,
      "end_time": 1716.04
    },
    {
      "text": " operate in the latent space. So what do we mean by latent space? Do you have an idea",
      "start_time": 1716.04,
      "end_time": 1720.88
    },
    {
      "text": " about it? What is the latent space? That is nothing but the feature vector right, feature",
      "start_time": 1720.88,
      "end_time": 1735.36
    },
    {
      "text": "",
      "start_time": 1735.36,
      "end_time": 1710.0
    },
    {
      "text": " vector of the image.",
      "start_time": 1711.0,
      "end_time": null
    },
    {
      "text": " m cross n resolution image converted into a latent vector which is very small in size.",
      "start_time": 1740.0,
      "end_time": 1759.4
    },
    {
      "text": " This is the latent vector. Suppose you have a 500 cross 500 into 3 channels RGB channels,",
      "start_time": 1759.4,
      "end_time": 1768.32
    },
    {
      "text": "",
      "start_time": 1768.32,
      "end_time": 1740.0
    },
    {
      "text": " So you can convert it into",
      "start_time": 1742.0,
      "end_time": null
    },
    {
      "text": " let us say 4 x 64 x 64 this is still a very small vector space. This is 500 x 500 x 3",
      "start_time": 1770.0,
      "end_time": 1781.08
    },
    {
      "text": " using an encoder convert it into a latent vector. For example, I have the image of a",
      "start_time": 1781.08,
      "end_time": 1788.2
    },
    {
      "text": " face. The image of a face converting to latent vector means one dimension of the latent vector",
      "start_time": 1788.2,
      "end_time": 1794.56
    },
    {
      "text": "",
      "start_time": 1794.56,
      "end_time": 1770.0
    },
    {
      "text": " can capture skin tone from the face you do not need all pixels of the face.",
      "start_time": 1775.28,
      "end_time": null
    },
    {
      "text": " I just need the skin tone so that can be one or two dimensions representing the skin tone.",
      "start_time": 1800.0,
      "end_time": 1806.36
    },
    {
      "text": " If there are too many skin tones possible then one bit may not be sufficient.",
      "start_time": 1806.36,
      "end_time": 1810.32
    },
    {
      "text": " So you may need the multiple bits to represent skin tone so that is this and then you want",
      "start_time": 1810.32,
      "end_time": 1815.18
    },
    {
      "text": " to talk about the hair color so that is another type and then you want to speak about the",
      "start_time": 1815.18,
      "end_time": 1820.88
    },
    {
      "text": " expression so that is another like this you are going to capture the feature vectors from",
      "start_time": 1820.88,
      "end_time": 1825.6
    },
    {
      "text": "",
      "start_time": 1825.6,
      "end_time": 1800.0
    },
    {
      "text": " the face that is what is the encoder trying to do.",
      "start_time": 1804.4,
      "end_time": null
    },
    {
      "text": " feature maps we say right in conventional neural network it generates the feature map.",
      "start_time": 1830.0,
      "end_time": 1834.24
    },
    {
      "text": " The feature map is nothing but a latent vector, it produces a vector which consists of your",
      "start_time": 1834.24,
      "end_time": 1840.4
    },
    {
      "text": " facial pictures or the features and that can be produced as a distribution, the skin distribution,",
      "start_time": 1840.4,
      "end_time": 1848.4
    },
    {
      "text": " the hair color distribution, it can generate distribution of the various people in terms",
      "start_time": 1848.4,
      "end_time": 1855.04
    },
    {
      "text": " of the skin, in terms of the hair and all of that various distribution can be produced",
      "start_time": 1855.04,
      "end_time": 1858.92
    },
    {
      "text": "",
      "start_time": 1858.92,
      "end_time": 1830.0
    },
    {
      "text": " or directly you see.",
      "start_time": 1831.08,
      "end_time": null
    },
    {
      "text": " for this phase this is the latent vector direct okay. A latent vector representation for phase",
      "start_time": 1860.0,
      "end_time": 1864.96
    },
    {
      "text": " 1, latent vector representation for phase 2 means that it is a feature vector. As an",
      "start_time": 1864.96,
      "end_time": 1869.72
    },
    {
      "text": " example for phase the feature vector for phase consists of capturing skin tone, hair color,",
      "start_time": 1869.72,
      "end_time": 1876.0
    },
    {
      "text": " smile, pose and so many things, eyes whatever you want to characterize as. So depending on",
      "start_time": 1876.0,
      "end_time": 1881.44
    },
    {
      "text": " that you can define the latent vector space and basically it is a low dimensional vector",
      "start_time": 1881.44,
      "end_time": 1888.0
    },
    {
      "text": "",
      "start_time": 1888.0,
      "end_time": 1860.0
    },
    {
      "text": " drawn from the high dimensional.",
      "start_time": 1862.0,
      "end_time": null
    },
    {
      "text": " image that is what we mean by latent vector is that clear in all of the deep learning",
      "start_time": 1890.0,
      "end_time": 1895.28
    },
    {
      "text": " techniques we will be operating with latent vectors. So, this is what is going to happen",
      "start_time": 1895.28,
      "end_time": 1899.88
    },
    {
      "text": " and then given the latent vector you can produce the image given this latent vector you can",
      "start_time": 1899.88,
      "end_time": 1909.52
    },
    {
      "text": " produce the image again this is the decoder ok.",
      "start_time": 1909.52,
      "end_time": 1916.0
    },
    {
      "text": "",
      "start_time": 1916.0,
      "end_time": 1890.0
    },
    {
      "text": " the image you call out the features into a latent vector and this latent vector is the",
      "start_time": 1897.0,
      "end_time": null
    },
    {
      "text": " vector produced given to decoder will actually reproduce that image back. So, this is called",
      "start_time": 1920.0,
      "end_time": 1925.72
    },
    {
      "text": " as an identity function, it produces an image back. So, this function is called as an identity",
      "start_time": 1925.72,
      "end_time": 1932.36
    },
    {
      "text": " function. What type of architecture is this? It is an autoencoder, this is an autoencoder,",
      "start_time": 1932.36,
      "end_time": 1942.84
    },
    {
      "text": "",
      "start_time": 1942.84,
      "end_time": 1920.0
    },
    {
      "text": " autoencoder. Autoencoder's job is to produce an identity function.",
      "start_time": 1925.44,
      "end_time": null
    },
    {
      "text": " Is this clear? Now, I was talking about in the diffusion models, the forward and reverse",
      "start_time": 1950.0,
      "end_time": 1957.58
    },
    {
      "text": " diffusion process. If the forward and reverse diffusion process applies noise on the images,",
      "start_time": 1957.58,
      "end_time": 1965.96
    },
    {
      "text": " the latent diffusion models apply noise on the latent vector instead of the image space,",
      "start_time": 1965.96,
      "end_time": 1971.32
    },
    {
      "text": " that is the difference. You understand? Instead of applying noise on this image, we apply noise",
      "start_time": 1971.32,
      "end_time": 1976.24
    },
    {
      "text": "",
      "start_time": 1976.24,
      "end_time": 1950.0
    },
    {
      "text": " from the latent vector which is a low dimensional representation of the given vector.",
      "start_time": 1953.76,
      "end_time": null
    },
    {
      "text": " than input that is it as simple as that. So, that is called as a latent diffusion model.",
      "start_time": 1980.0,
      "end_time": 1985.56
    },
    {
      "text": " If you operate in the latent space it is called as latent diffusion model. If you operate",
      "start_time": 1985.56,
      "end_time": 1990.56
    },
    {
      "text": " in the image space it is just called as a diffusion model. This is clear is the difference",
      "start_time": 1990.56,
      "end_time": 1995.08
    },
    {
      "text": " clear. Now, how are you going to generate this latent vector you can use any encoder decoder",
      "start_time": 1995.08,
      "end_time": 2000.12
    },
    {
      "text": " model one of the examples is auto encoder ok. And the VAE is different how is auto encoder",
      "start_time": 2000.12,
      "end_time": 2007.28
    },
    {
      "text": "",
      "start_time": 2007.28,
      "end_time": 1980.0
    },
    {
      "text": " different from that of the VAE.",
      "start_time": 1982.32,
      "end_time": null
    },
    {
      "text": " You must have studied in deep learning right or this is not there, is this there or not",
      "start_time": 2010.0,
      "end_time": 2018.32
    },
    {
      "text": " there, it is there, tell me.",
      "start_time": 2018.32,
      "end_time": 2020.88
    },
    {
      "text": " VIE, the difference between autoencoder and variational autoencoder is that it directly",
      "start_time": 2020.88,
      "end_time": 2030.52
    },
    {
      "text": " from the given image directly a latent vector if it is produced, it is called as autoencoder.",
      "start_time": 2030.52,
      "end_time": 2036.28
    },
    {
      "text": "",
      "start_time": 2036.28,
      "end_time": 2010.0
    },
    {
      "text": " it produces only the mu and sigma distributed.",
      "start_time": 2013.76,
      "end_time": null
    },
    {
      "text": " distribution if it produces skin color distribution for set of faces. I told you about the hair",
      "start_time": 2040.0,
      "end_time": 2045.84
    },
    {
      "text": " color distribution and then eyes color distribution. So if it produces different distribution instead",
      "start_time": 2045.84,
      "end_time": 2053.08
    },
    {
      "text": " of directly the vector, instead of directly the vector if it produces a distribution and",
      "start_time": 2053.08,
      "end_time": 2059.0
    },
    {
      "text": " from this distribution you produce a latent vector then it is called as variational auto",
      "start_time": 2059.0,
      "end_time": 2064.52
    },
    {
      "text": "",
      "start_time": 2064.52,
      "end_time": 2040.0
    },
    {
      "text": " encoder and in variational order encoder there is some reverse back",
      "start_time": 2040.0,
      "end_time": null
    },
    {
      "text": " propagation step but it breaks here because from the distribution you are generating you",
      "start_time": 2070.0,
      "end_time": 2074.68
    },
    {
      "text": " are sampling the latent vector.",
      "start_time": 2074.68,
      "end_time": 2076.48
    },
    {
      "text": " Directly you will not produce a latent vector I hope you are getting what I am saying you",
      "start_time": 2076.48,
      "end_time": 2080.64
    },
    {
      "text": " are generating the output of the encoder if you do not understand too much about it you",
      "start_time": 2080.64,
      "end_time": 2085.68
    },
    {
      "text": " just understand this that in the auto encoder directly low dimensional vector is produced",
      "start_time": 2085.68,
      "end_time": 2091.0
    },
    {
      "text": " for the image.",
      "start_time": 2091.0,
      "end_time": 2092.4
    },
    {
      "text": " In the case of variational auto encoder it produces a distribution values from which you",
      "start_time": 2092.4,
      "end_time": 2097.2
    },
    {
      "text": "",
      "start_time": 2097.2,
      "end_time": 2070.0
    },
    {
      "text": " you will sample and produce a latent vector.",
      "start_time": 2071.92,
      "end_time": null
    },
    {
      "text": " You will sample from this distribution and produce a latent vector corresponding to the",
      "start_time": 2100.0,
      "end_time": 2103.84
    },
    {
      "text": " image.",
      "start_time": 2103.84,
      "end_time": 2104.84
    },
    {
      "text": " So, directly the latent vector is not given.",
      "start_time": 2104.84,
      "end_time": 2107.68
    },
    {
      "text": " So, there will be some gap in this operation.",
      "start_time": 2107.68,
      "end_time": 2110.68
    },
    {
      "text": " So, there is something called reparameterization trick and all that is used in VAE which is",
      "start_time": 2110.68,
      "end_time": 2115.56
    },
    {
      "text": " a very important concept that you should know.",
      "start_time": 2115.56,
      "end_time": 2117.88
    },
    {
      "text": " So, when you are studying you look at that reparameterization trick and also that KL",
      "start_time": 2117.88,
      "end_time": 2123.36
    },
    {
      "text": " divergence and then lot of operations here to be performed to make the VAE very powerful.",
      "start_time": 2123.36,
      "end_time": 2129.12
    },
    {
      "text": "",
      "start_time": 2129.12,
      "end_time": 2100.0
    },
    {
      "text": " So that's it.",
      "start_time": 2102.0,
      "end_time": null
    },
    {
      "text": " that you need to study if you are studying VAE but the reason why I am talking about",
      "start_time": 2130.0,
      "end_time": 2134.48
    },
    {
      "text": " VAE is this is one of the first steps in stable diffusion it uses VAE.",
      "start_time": 2134.48,
      "end_time": 2139.4
    },
    {
      "text": " So, I was under the assumption that you will know about it so I did not want to discuss",
      "start_time": 2139.4,
      "end_time": 2143.28
    },
    {
      "text": " this in too much detail of the math involved in it but you should know about it.",
      "start_time": 2143.28,
      "end_time": 2147.2
    },
    {
      "text": " It is a very important aspect, reparameterization trick you should know in VAE and also the",
      "start_time": 2147.2,
      "end_time": 2152.48
    },
    {
      "text": " KL divergence that you should know.",
      "start_time": 2152.48,
      "end_time": 2154.68
    },
    {
      "text": " So, this is how the latent vector is produced.",
      "start_time": 2154.68,
      "end_time": 2157.36
    },
    {
      "text": "",
      "start_time": 2157.36,
      "end_time": 2130.0
    },
    {
      "text": " So, for our discussion in the LLM class, we will be discussing the LLM class.",
      "start_time": 2132.64,
      "end_time": null
    },
    {
      "text": " we just need to know that the input image produces a distribution from which a latent",
      "start_time": 2160.0,
      "end_time": 2165.94
    },
    {
      "text": " vector is produced and this latent vector is further used in the forward and reverse",
      "start_time": 2165.94,
      "end_time": 2170.96
    },
    {
      "text": " processes for generating images.",
      "start_time": 2170.96,
      "end_time": 2173.08
    },
    {
      "text": " Is this clear?",
      "start_time": 2173.08,
      "end_time": 2175.72
    },
    {
      "text": " So far is this clear?",
      "start_time": 2175.72,
      "end_time": 2176.96
    },
    {
      "text": " What is the diffusion model?",
      "start_time": 2176.96,
      "end_time": 2178.68
    },
    {
      "text": " What is the forward process?",
      "start_time": 2178.68,
      "end_time": 2180.32
    },
    {
      "text": " What is the reverse process?",
      "start_time": 2180.32,
      "end_time": 2182.04
    },
    {
      "text": " What is the characteristic equation that defines the generation of the next image, the probability",
      "start_time": 2182.04,
      "end_time": 2186.6
    },
    {
      "text": "",
      "start_time": 2186.6,
      "end_time": 2160.0
    },
    {
      "text": " function and what is the characteristic equation that defines the reverse function.",
      "start_time": 2163.36,
      "end_time": null
    },
    {
      "text": " process. These aspects should be clear and we do not operate in the image space, we operate",
      "start_time": 2190.0,
      "end_time": 2195.6
    },
    {
      "text": " in the latent space because the image space is very big and too many noise schedules,",
      "start_time": 2195.6,
      "end_time": 2201.08
    },
    {
      "text": " the computation cost is very very high and therefore, we only operate in the latent",
      "start_time": 2201.08,
      "end_time": 2205.68
    },
    {
      "text": " space and then at the last step only we convert to image space.",
      "start_time": 2205.68,
      "end_time": 2209.28
    },
    {
      "text": " The starting step is image space, the last step is image space and all the in between",
      "start_time": 2209.28,
      "end_time": 2213.8
    },
    {
      "text": " forward and reverse steps are carried out only in the latent space, is this clear? That",
      "start_time": 2213.8,
      "end_time": 2218.4
    },
    {
      "text": "",
      "start_time": 2218.4,
      "end_time": 2190.0
    },
    {
      "text": " That is why this model is called as.",
      "start_time": 2191.4,
      "end_time": null
    },
    {
      "text": " latent diffusion model which is used in stable diffusion. This is clear. Now, we will proceed",
      "start_time": 2220.0,
      "end_time": 2225.4
    },
    {
      "text": " to stable diffusion. This is the theory that was needed. Stable diffusion, stable diffusion",
      "start_time": 2225.4,
      "end_time": 2245.36
    },
    {
      "text": "",
      "start_time": 2245.36,
      "end_time": 2220.0
    },
    {
      "text": " is a architectural design for generating images.",
      "start_time": 2224.08,
      "end_time": null
    },
    {
      "text": " This is a recent, recent almost 5 years back I think it is introduced, Dali and all of",
      "start_time": 2250.0,
      "end_time": 2258.4
    },
    {
      "text": " that they introduced first image generation. They are all based on diffusion models. This",
      "start_time": 2258.4,
      "end_time": 2262.52
    },
    {
      "text": " is not a transformer model. This does not use transformer, it uses diffusion models",
      "start_time": 2262.52,
      "end_time": 2268.32
    },
    {
      "text": " to generate images. This is different in that sense. So, its first step is to generate the",
      "start_time": 2268.32,
      "end_time": 2277.88
    },
    {
      "text": "",
      "start_time": 2277.88,
      "end_time": 2250.0
    },
    {
      "text": " latent vector.",
      "start_time": 2250.88,
      "end_time": null
    },
    {
      "text": " The first step in latent stable diffusion for generating images is to produce a latent",
      "start_time": 2280.0,
      "end_time": 2288.4
    },
    {
      "text": " vector and to produce a latent vector it uses VAE, variational autoencoder. It uses variational",
      "start_time": 2288.4,
      "end_time": 2297.84
    },
    {
      "text": " autoencoder to generate a latent vector for any given image. You have to train the stable",
      "start_time": 2297.84,
      "end_time": 2304.24
    },
    {
      "text": " diffusion model. To train the stable diffusion model, first of all you need for it to generate",
      "start_time": 2304.24,
      "end_time": 2309.72
    },
    {
      "text": "",
      "start_time": 2309.72,
      "end_time": 2280.0
    },
    {
      "text": " be made",
      "start_time": 2282.0,
      "end_time": null
    },
    {
      "text": " at a later stage you need to train the model on set of images first okay that training",
      "start_time": 2310.0,
      "end_time": 2315.84
    },
    {
      "text": " process that I am talking about you are given a set of images and these images latent vector",
      "start_time": 2315.84,
      "end_time": 2321.64
    },
    {
      "text": " representations you need to produce using the diffusion I mean using the VAE variational",
      "start_time": 2321.64,
      "end_time": 2327.96
    },
    {
      "text": " auto encoder models okay.",
      "start_time": 2327.96,
      "end_time": 2330.48
    },
    {
      "text": " Then the next step is to add the noise and figure out how much noise has been added so",
      "start_time": 2330.48,
      "end_time": 2339.04
    },
    {
      "text": "",
      "start_time": 2339.04,
      "end_time": 2310.0
    },
    {
      "text": " So add noise.",
      "start_time": 2310.84,
      "end_time": null
    },
    {
      "text": " That is the forward process, add noise and predict how much noise is added.",
      "start_time": 2340.0,
      "end_time": 2359.24
    },
    {
      "text": "",
      "start_time": 2359.24,
      "end_time": 2340.0
    },
    {
      "text": " I will call this as third step in between sometimes there is conditioning done, conditioning",
      "start_time": 2347.16,
      "end_time": 2340.0
    },
    {
      "text": " Thank you. Thank you.",
      "start_time": 2341.0,
      "end_time": 2342.0
    },
    {
      "text": " Thank you.",
      "start_time": 2342.0,
      "end_time": 2343.0
    },
    {
      "text": " Thank you.",
      "start_time": 2343.0,
      "end_time": 2344.0
    },
    {
      "text": " Thank you.",
      "start_time": 2344.0,
      "end_time": 2345.0
    },
    {
      "text": " Thank you.",
      "start_time": 2345.0,
      "end_time": 2346.0
    },
    {
      "text": " Thank you.",
      "start_time": 2346.0,
      "end_time": 2347.0
    },
    {
      "text": " Thank you.",
      "start_time": 2347.0,
      "end_time": 2348.0
    },
    {
      "text": " Thank you.",
      "start_time": 2348.0,
      "end_time": 2349.0
    },
    {
      "text": " Thank you.",
      "start_time": 2349.0,
      "end_time": 2350.0
    },
    {
      "text": " Thank you.",
      "start_time": 2350.0,
      "end_time": 2351.0
    },
    {
      "text": " Thank you.",
      "start_time": 2351.0,
      "end_time": 2352.0
    },
    {
      "text": " Thank you.",
      "start_time": 2352.0,
      "end_time": 2353.0
    },
    {
      "text": " Thank you.",
      "start_time": 2353.0,
      "end_time": 2354.0
    },
    {
      "text": " Thank you.",
      "start_time": 2354.0,
      "end_time": 2355.0
    },
    {
      "text": " Thank you.",
      "start_time": 2355.0,
      "end_time": 2356.0
    },
    {
      "text": " Thank you.",
      "start_time": 2356.0,
      "end_time": 2357.0
    },
    {
      "text": " Thank you.",
      "start_time": 2357.0,
      "end_time": 2358.0
    },
    {
      "text": " Thank you.",
      "start_time": 2358.0,
      "end_time": 2359.0
    },
    {
      "text": " Thank you.",
      "start_time": 2359.0,
      "end_time": 2360.0
    },
    {
      "text": " Thank you.",
      "start_time": 2360.0,
      "end_time": 2361.0
    },
    {
      "text": " Thank you.",
      "start_time": 2361.0,
      "end_time": 2362.0
    },
    {
      "text": " Thank you.",
      "start_time": 2362.0,
      "end_time": 2363.0
    },
    {
      "text": " Thank you.",
      "start_time": 2363.0,
      "end_time": 2364.0
    },
    {
      "text": " Thank you.",
      "start_time": 2364.0,
      "end_time": 2365.0
    },
    {
      "text": " Thank you.",
      "start_time": 2365.0,
      "end_time": 2366.0
    },
    {
      "text": " Thank you.",
      "start_time": 2366.0,
      "end_time": 2367.0
    },
    {
      "text": " Thank you.",
      "start_time": 2367.0,
      "end_time": 2368.0
    },
    {
      "text": " Thank you.",
      "start_time": 2368.0,
      "end_time": 2369.0
    },
    {
      "text": " using text prompt and for this it uses contrastive learning image pre-training clip and this",
      "start_time": 2370.0,
      "end_time": 2387.16
    },
    {
      "text": " uses a unit architecture.",
      "start_time": 2387.16,
      "end_time": 2389.36
    },
    {
      "text": "",
      "start_time": 2389.36,
      "end_time": 2370.0
    },
    {
      "text": " The second step is conditioning using text prompt. Contrasting.",
      "start_time": 2380.8,
      "end_time": 2381.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2381.8,
      "end_time": 2382.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2382.8,
      "end_time": 2383.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2383.8,
      "end_time": 2384.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2384.8,
      "end_time": 2385.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2385.8,
      "end_time": 2386.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2386.8,
      "end_time": 2387.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2387.8,
      "end_time": 2388.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2388.8,
      "end_time": 2389.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2389.8,
      "end_time": 2390.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2390.8,
      "end_time": 2391.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2391.8,
      "end_time": 2392.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2392.8,
      "end_time": 2393.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2393.8,
      "end_time": 2394.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2394.8,
      "end_time": 2395.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2395.8,
      "end_time": 2396.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2396.8,
      "end_time": 2397.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2397.8,
      "end_time": 2398.8
    },
    {
      "text": " Contrasting.",
      "start_time": 2398.8,
      "end_time": 2399.8
    },
    {
      "text": " learning image pre-training architecture is used, I will explain that. These are the three",
      "start_time": 2400.0,
      "end_time": 2404.4
    },
    {
      "text": " steps in stable diffusion training. This is a training process.",
      "start_time": 2404.4,
      "end_time": 2425.56
    },
    {
      "text": "",
      "start_time": 2425.56,
      "end_time": 2400.0
    },
    {
      "text": " is a encoder decoder architecture there is no expansion",
      "start_time": 2404.4,
      "end_time": null
    },
    {
      "text": " for that, it is a U architecture, U type of architecture, U convolutions and deconvolution,",
      "start_time": 2430.0,
      "end_time": 2438.8
    },
    {
      "text": " decoder that is called a Unet. If there is any name for U, I am not sure about that.",
      "start_time": 2438.8,
      "end_time": 2446.8
    },
    {
      "text": " It is in a form of U. It is also like a transformer, clip is also a transformer. VAE is CNM,",
      "start_time": 2446.8,
      "end_time": 2459.24
    },
    {
      "text": "",
      "start_time": 2459.24,
      "end_time": 2430.0
    },
    {
      "text": " is a CNN.",
      "start_time": 2430.8,
      "end_time": null
    },
    {
      "text": " unit also is cnn but it is like transformer. These are the three steps. So what is the",
      "start_time": 2460.0,
      "end_time": 2471.0
    },
    {
      "text": " first step? VAA just now I wrote the picture. So you take an image and produce the latent",
      "start_time": 2471.0,
      "end_time": 2480.4
    },
    {
      "text": "",
      "start_time": 2480.4,
      "end_time": 2460.0
    },
    {
      "text": " So, it is a latent vector encoder.",
      "start_time": 2463.36,
      "end_time": null
    },
    {
      "text": " You can use a pre-trained VIE, pre-trained VIE so that it is able to generate the image,",
      "start_time": 2490.0,
      "end_time": 2501.96
    },
    {
      "text": " it is an identity function I told you.",
      "start_time": 2501.96,
      "end_time": 2503.84
    },
    {
      "text": " What does that mean?",
      "start_time": 2503.84,
      "end_time": 2504.84
    },
    {
      "text": " If image X1 is given, it produces X1, X1 prime which is as close to X1.",
      "start_time": 2504.84,
      "end_time": 2511.88
    },
    {
      "text": " So X1 will be approximately equal to X1 dash, it is an identity function.",
      "start_time": 2511.88,
      "end_time": 2518.48
    },
    {
      "text": "",
      "start_time": 2518.48,
      "end_time": 2490.0
    },
    {
      "text": " So again,",
      "start_time": 2492.0,
      "end_time": null
    },
    {
      "text": " This is the encoder, this is the encoder, this is the decoder, decoder, this is the VAE architecture.",
      "start_time": 2520.0,
      "end_time": 2529.12
    },
    {
      "text": " The encoder convolution layer produces from the given image latent vector representation,",
      "start_time": 2529.12,
      "end_time": 2537.04
    },
    {
      "text": " from the latent vector representation you give it to decoder produces x1 dash.",
      "start_time": 2537.04,
      "end_time": 2542.0
    },
    {
      "text": " This is the first step, we are using a pre-tent VAE, so it automatically produces the latent",
      "start_time": 2542.0,
      "end_time": 2547.16
    },
    {
      "text": "",
      "start_time": 2547.16,
      "end_time": 2520.0
    },
    {
      "text": " How do you know that?",
      "start_time": 2522.84,
      "end_time": null
    },
    {
      "text": " that the latent vector is correct because it produces back x1 dash which is as close",
      "start_time": 2550.0,
      "end_time": 2553.56
    },
    {
      "text": " to the original distribution. So, it is the correct representation of the image. There",
      "start_time": 2553.56,
      "end_time": 2557.84
    },
    {
      "text": " is not much of loss in the information here. So, we are going to pick up this latent vector.",
      "start_time": 2557.84,
      "end_time": 2562.2
    },
    {
      "text": " This is the first step that is this latent vector. So, what is the next step? See in",
      "start_time": 2562.2,
      "end_time": 2566.72
    },
    {
      "text": " the next step, we have to condition based on the prompt.",
      "start_time": 2566.72,
      "end_time": 2576.24
    },
    {
      "text": "",
      "start_time": 2576.24,
      "end_time": 2550.0
    },
    {
      "text": " So, we are going to say that I want to say that I want to say that I want to say that",
      "start_time": 2557.0,
      "end_time": null
    },
    {
      "text": " want to this is a cat image so it is a pair the image is that of a cat I do not know how",
      "start_time": 2580.0,
      "end_time": 2594.48
    },
    {
      "text": " to write the cat and all that so this is cat okay so this is a cat image this is a text",
      "start_time": 2594.48,
      "end_time": 2602.92
    },
    {
      "text": "",
      "start_time": 2602.92,
      "end_time": 2580.0
    },
    {
      "text": " This is a text prompt.",
      "start_time": 2587.0,
      "end_time": null
    },
    {
      "text": " It goes through clip, contrastive, learning, image, retraining.",
      "start_time": 2610.0,
      "end_time": 2625.44
    },
    {
      "text": "",
      "start_time": 2625.44,
      "end_time": 2610.0
    },
    {
      "text": " is the transform on model used.",
      "start_time": 2613.16,
      "end_time": null
    },
    {
      "text": " I will slightly digress now, let us see how clip is trained on.",
      "start_time": 2640.0,
      "end_time": 2644.76
    },
    {
      "text": " In stable diffusion we are using a pre-trained clip, but how is the training done for clip",
      "start_time": 2644.76,
      "end_time": 2650.28
    },
    {
      "text": " itself we will see now.",
      "start_time": 2650.28,
      "end_time": 2652.52
    },
    {
      "text": " The clip uses two architectures, clip uses two architectures.",
      "start_time": 2652.52,
      "end_time": 2660.32
    },
    {
      "text": " I am now talking about multimodal LLM, slightly degress from image generation to multimodal",
      "start_time": 2660.32,
      "end_time": 2665.64
    },
    {
      "text": " LLM.",
      "start_time": 2665.64,
      "end_time": 2666.64
    },
    {
      "text": "",
      "start_time": 2666.64,
      "end_time": 2640.0
    },
    {
      "text": " Clip is an example of a multi-modal LLM.",
      "start_time": 2642.64,
      "end_time": null
    },
    {
      "text": " What is a multimodal LLM?",
      "start_time": 2670.0,
      "end_time": 2683.92
    },
    {
      "text": " It access other than text input other types of data like in this example image, image",
      "start_time": 2683.92,
      "end_time": 2691.08
    },
    {
      "text": " it access, text it access, generates text, generates text embedding that is what we mean",
      "start_time": 2691.08,
      "end_time": 2697.44
    },
    {
      "text": " by multimodal LLM.",
      "start_time": 2697.44,
      "end_time": 2699.08
    },
    {
      "text": "",
      "start_time": 2699.08,
      "end_time": 2670.0
    },
    {
      "text": " It takes different...",
      "start_time": 2670.9,
      "end_time": null
    },
    {
      "text": " types of inputs in generating of embeddings that is called as a multimodal LLM.",
      "start_time": 2700.0,
      "end_time": 2705.28
    },
    {
      "text": " So here clip accepts image text pairs, image text pairs.",
      "start_time": 2705.28,
      "end_time": 2715.12
    },
    {
      "text": " See there is slight deviation from the original idea that is it but it has become very popular.",
      "start_time": 2715.12,
      "end_time": 2722.6
    },
    {
      "text": " You must have heard about clip blip and all of that right but it became popular.",
      "start_time": 2722.6,
      "end_time": 2726.6
    },
    {
      "text": "",
      "start_time": 2726.6,
      "end_time": 2700.0
    },
    {
      "text": " What did they do? I will tell you now, just now.",
      "start_time": 2701.0,
      "end_time": 2702.0
    },
    {
      "text": " Just in a minute.",
      "start_time": 2702.0,
      "end_time": 2703.0
    },
    {
      "text": " So here they have used a vision transformer.",
      "start_time": 2730.0,
      "end_time": 2734.26
    },
    {
      "text": " They have used a vision transformer.",
      "start_time": 2734.26,
      "end_time": 2736.82
    },
    {
      "text": " What is the vision transformer we have studied already?",
      "start_time": 2736.82,
      "end_time": 2739.02
    },
    {
      "text": " What is it?",
      "start_time": 2739.02,
      "end_time": 2742.42
    },
    {
      "text": " Given an image it fits it into patches.",
      "start_time": 2742.42,
      "end_time": 2746.94
    },
    {
      "text": " Transformer it expects sequence.",
      "start_time": 2746.94,
      "end_time": 2747.94
    },
    {
      "text": " This you should remember forever.",
      "start_time": 2747.94,
      "end_time": 2751.14
    },
    {
      "text": " Transformer means it is a sequence to sequence model.",
      "start_time": 2751.14,
      "end_time": 2753.7
    },
    {
      "text": " We should give some sequence to transformer.",
      "start_time": 2753.7,
      "end_time": 2756.22
    },
    {
      "text": " To give a sequence image is not a sequence basically.",
      "start_time": 2756.22,
      "end_time": 2759.42
    },
    {
      "text": "",
      "start_time": 2759.42,
      "end_time": 2730.0
    },
    {
      "text": " How do you convert?",
      "start_time": 2730.6,
      "end_time": null
    },
    {
      "text": " an image into a sequence, piece it, scissor it and then make the sequence and produce",
      "start_time": 2760.0,
      "end_time": 2765.92
    },
    {
      "text": " that sequence. So, image sequence is given, image sequence is given and it produces image",
      "start_time": 2765.92,
      "end_time": 2773.28
    },
    {
      "text": " embeddings, image embeddings. This is one architecture used in clip. On the other side",
      "start_time": 2773.28,
      "end_time": 2782.44
    },
    {
      "text": "",
      "start_time": 2782.44,
      "end_time": 2760.0
    },
    {
      "text": " it uses GPT kind of architecture decoder, GPT type of decoder.",
      "start_time": 2764.9,
      "end_time": null
    },
    {
      "text": " and text prompt is given and it produces text embeddings.",
      "start_time": 2790.0,
      "end_time": 2798.08
    },
    {
      "text": " Text embeddings are produced, two architectures separately in the clip model and these image",
      "start_time": 2798.08,
      "end_time": 2805.4
    },
    {
      "text": " embeddings and text embeddings let us say we are giving the image of a cat, image of",
      "start_time": 2805.4,
      "end_time": 2811.28
    },
    {
      "text": " a cat and the text is also saying cat image, text is also saying cat image.",
      "start_time": 2811.28,
      "end_time": 2818.36
    },
    {
      "text": "",
      "start_time": 2818.36,
      "end_time": 2790.0
    },
    {
      "text": " So the embedding generated by this is the same as the previous one.",
      "start_time": 2797.0,
      "end_time": null
    },
    {
      "text": " these two should be close, they have to match right, it is a match pair, image and text",
      "start_time": 2820.0,
      "end_time": 2825.52
    },
    {
      "text": " match pair. So if it is not matching it will retrain saying that these embedding should",
      "start_time": 2825.52,
      "end_time": 2831.16
    },
    {
      "text": " come closer to each other. So checks the cosine similarity between the two that is why it",
      "start_time": 2831.16,
      "end_time": 2835.84
    },
    {
      "text": " is called as contrastive learning, cosine similarity between the text embedding and",
      "start_time": 2835.84,
      "end_time": 2842.04
    },
    {
      "text": " image embedding and if it is not close to one there is lot of error because we know",
      "start_time": 2842.04,
      "end_time": 2847.68
    },
    {
      "text": "",
      "start_time": 2847.68,
      "end_time": 2820.0
    },
    {
      "text": " that we have given matching.",
      "start_time": 2822.32,
      "end_time": null
    },
    {
      "text": " we gave and the text is also cat image. The two embeddings should be closed. If the two",
      "start_time": 2850.0,
      "end_time": 2855.2
    },
    {
      "text": " embeddings are not closed then there is high loss. High loss will back propagate and train",
      "start_time": 2855.2,
      "end_time": 2861.4
    },
    {
      "text": " so that the loss between the two embeddings is reduced. What does that mean? It learns",
      "start_time": 2861.4,
      "end_time": 2866.44
    },
    {
      "text": " the equivalent text embedding and image embedding it brings them to same space. That is what",
      "start_time": 2866.44,
      "end_time": 2872.04
    },
    {
      "text": " the clip is trying to learn. Suppose I give that this is a dog image is a text prompt",
      "start_time": 2872.04,
      "end_time": 2879.0
    },
    {
      "text": "",
      "start_time": 2879.0,
      "end_time": 2850.0
    },
    {
      "text": " But what I am giving...",
      "start_time": 2851.0,
      "end_time": null
    },
    {
      "text": " is a cat and now you check whether the embedding are really dissimilar or not is there actually",
      "start_time": 2880.0,
      "end_time": 2887.84
    },
    {
      "text": " very similar then there is again there is a loss. So you train so that they become far",
      "start_time": 2887.84,
      "end_time": 2892.6
    },
    {
      "text": " apart so for similarity you can keep it one for dissimilarity you can keep it zero any",
      "start_time": 2892.6,
      "end_time": 2897.64
    },
    {
      "text": " output you can keep. So if they are not close to this if they are similar they have to be",
      "start_time": 2897.64,
      "end_time": 2901.96
    },
    {
      "text": " close to one if they are not close to one there is a loss if they are dissimilar information",
      "start_time": 2901.96,
      "end_time": 2907.76
    },
    {
      "text": "",
      "start_time": 2907.76,
      "end_time": 2880.0
    },
    {
      "text": " they have to be close to zero if they are not close to zero.",
      "start_time": 2882.24,
      "end_time": null
    },
    {
      "text": " to 0 again there is a loss. So, like this they gave similar pairs dissimilar pairs like",
      "start_time": 2910.0,
      "end_time": 2915.44
    },
    {
      "text": " this a lot of may be 1 k 2 k 2 lakhs I am saying 2 lakh images and then they trained",
      "start_time": 2915.44,
      "end_time": 2921.52
    },
    {
      "text": " this transformer this is how clip is trained. Now, once the clip is trained now you can give",
      "start_time": 2921.52,
      "end_time": 2927.6
    },
    {
      "text": " that just the text and the text embedding generated would be in the space similar to",
      "start_time": 2927.6,
      "end_time": 2933.72
    },
    {
      "text": " that of the image. You understand the difference between the",
      "start_time": 2933.72,
      "end_time": 2937.04
    },
    {
      "text": "",
      "start_time": 2937.04,
      "end_time": 2910.0
    },
    {
      "text": " birth embedding and this also text embedding it can do.",
      "start_time": 2912.56,
      "end_time": null
    },
    {
      "text": " So, once the clip is ready you can just give text prompt and you can get the text embeddings.",
      "start_time": 2940.0,
      "end_time": 2947.36
    },
    {
      "text": " What is the difference between clips text embedding and birds bird same text what is",
      "start_time": 2947.36,
      "end_time": 2955.32
    },
    {
      "text": " the difference the embedded same sentence let us say cat image cat image to bird cat",
      "start_time": 2955.32,
      "end_time": 2963.04
    },
    {
      "text": " image to clip if you give the embedding generated by the clip will be similar to image embedding",
      "start_time": 2963.04,
      "end_time": 2969.68
    },
    {
      "text": "",
      "start_time": 2969.68,
      "end_time": 2940.0
    },
    {
      "text": " Thanks, please.",
      "start_time": 2941.0,
      "end_time": null
    },
    {
      "text": " The two embeddings will not be same.",
      "start_time": 2970.0,
      "end_time": 2973.16
    },
    {
      "text": " The training tasks are different.",
      "start_time": 2973.16,
      "end_time": 2976.36
    },
    {
      "text": " Do you understand?",
      "start_time": 2976.36,
      "end_time": 2977.36
    },
    {
      "text": " The birds has been trained only on missing word prediction.",
      "start_time": 2977.36,
      "end_time": 2981.12
    },
    {
      "text": " The way the embeddings are generated by birds is totally different.",
      "start_time": 2981.12,
      "end_time": 2984.68
    },
    {
      "text": " For the same sentence, do you understand the difference and can you appreciate?",
      "start_time": 2984.68,
      "end_time": 2990.16
    },
    {
      "text": " It is very important on what they have been trained on, what is the task at which they",
      "start_time": 2990.16,
      "end_time": 2995.36
    },
    {
      "text": " are good at and based on that only they will produce the embedding.",
      "start_time": 2995.36,
      "end_time": 2999.96
    },
    {
      "text": "",
      "start_time": 2999.96,
      "end_time": 2970.0
    },
    {
      "text": " Thank you.",
      "start_time": 2971.0,
      "end_time": null
    },
    {
      "text": " Although the sentences are same, the kind of embedding produced by BERT would be very",
      "start_time": 3000.0,
      "end_time": 3004.98
    },
    {
      "text": " different from the embedding produced by CLIP and the CLIP embedding would be close to image",
      "start_time": 3004.98,
      "end_time": 3011.06
    },
    {
      "text": " type of embedding.",
      "start_time": 3011.06,
      "end_time": 3013.1
    },
    {
      "text": " So you should know if you are operating in image related applications, the text prompt",
      "start_time": 3013.1,
      "end_time": 3017.28
    },
    {
      "text": " embedding should be close to image related embedding.",
      "start_time": 3017.28,
      "end_time": 3020.04
    },
    {
      "text": " You cannot use a BERT there.",
      "start_time": 3020.04,
      "end_time": 3022.34
    },
    {
      "text": " You understand when you give a text prompt and you want it to be converted into an embedding",
      "start_time": 3022.34,
      "end_time": 3027.1
    },
    {
      "text": "",
      "start_time": 3027.1,
      "end_time": 3000.0
    },
    {
      "text": " and you are using that for generating an image.",
      "start_time": 3002.32,
      "end_time": null
    },
    {
      "text": " Don't you think we should use clip now?",
      "start_time": 3030.0,
      "end_time": 3032.84
    },
    {
      "text": " Do you understand what I am saying?",
      "start_time": 3032.84,
      "end_time": 3034.96
    },
    {
      "text": " So the kind of prompt you gave cat image, cat image, here also cat image but this produce",
      "start_time": 3034.96,
      "end_time": 3044.4
    },
    {
      "text": " 0.1, 0.2, this may produce 0.6, 0.5.",
      "start_time": 3044.4,
      "end_time": 3048.92
    },
    {
      "text": " The two embeddings are different because the training that has been given is different.",
      "start_time": 3048.92,
      "end_time": 3053.6
    },
    {
      "text": " You get what I am saying.",
      "start_time": 3053.6,
      "end_time": 3056.48
    },
    {
      "text": "",
      "start_time": 3056.48,
      "end_time": 3030.0
    },
    {
      "text": " So 0.6, 0.5 will be similar to that of image embedding which is closed.",
      "start_time": 3033.52,
      "end_time": null
    },
    {
      "text": " And if you are talking about image applications you should use such pre-trained models.",
      "start_time": 3060.0,
      "end_time": 3065.52
    },
    {
      "text": " Now you understand hopefully which pre-trained model you should choose.",
      "start_time": 3065.52,
      "end_time": 3069.76
    },
    {
      "text": " It is not just like that some pre-trained model.",
      "start_time": 3069.76,
      "end_time": 3073.24
    },
    {
      "text": " Your first understanding should be what is it trained on?",
      "start_time": 3073.24,
      "end_time": 3077.76
    },
    {
      "text": " Is it going to serve my purpose or not?",
      "start_time": 3077.76,
      "end_time": 3080.2
    },
    {
      "text": " Do you understand?",
      "start_time": 3080.2,
      "end_time": 3081.2
    },
    {
      "text": " Lot of pre-trained models are available.",
      "start_time": 3081.2,
      "end_time": 3084.84
    },
    {
      "text": " Just like that you cannot pick up something.",
      "start_time": 3084.84,
      "end_time": 3086.88
    },
    {
      "text": "",
      "start_time": 3086.88,
      "end_time": 3060.0
    },
    {
      "text": " I use the VGG, I use the ResNet, like that you use.",
      "start_time": 3062.64,
      "end_time": null
    },
    {
      "text": " It is not generic, you have to decide what task it is trained on whether it is going",
      "start_time": 3090.0,
      "end_time": 3095.28
    },
    {
      "text": " to suit my application or not.",
      "start_time": 3095.28,
      "end_time": 3097.88
    },
    {
      "text": " Then it will be closer not that the bird will not work, but those embeddings will be far",
      "start_time": 3097.88,
      "end_time": 3102.1
    },
    {
      "text": " away from your image embeddings.",
      "start_time": 3102.1,
      "end_time": 3104.64
    },
    {
      "text": " They may not really serve the purpose so much.",
      "start_time": 3104.64,
      "end_time": 3107.08
    },
    {
      "text": " So, that is the importance of which pre-trained model.",
      "start_time": 3107.08,
      "end_time": 3112.0
    },
    {
      "text": " Pre-training means that you should know the task on which it is pre-trained.",
      "start_time": 3112.0,
      "end_time": 3117.2
    },
    {
      "text": "",
      "start_time": 3117.2,
      "end_time": 3090.0
    },
    {
      "text": " So that you know what kind of embeddings get generated from the...",
      "start_time": 3092.8,
      "end_time": null
    },
    {
      "text": " pre-trained model and whether those embeddings will make sense in my application or not and",
      "start_time": 3120.0,
      "end_time": 3125.04
    },
    {
      "text": " based on that you should proceed further.",
      "start_time": 3125.04,
      "end_time": 3127.16
    },
    {
      "text": " Is this clear?",
      "start_time": 3127.16,
      "end_time": 3128.16
    },
    {
      "text": " Okay.",
      "start_time": 3128.16,
      "end_time": 3129.16
    },
    {
      "text": " So, is clip clear?",
      "start_time": 3129.16,
      "end_time": 3130.16
    },
    {
      "text": " So, the pre-trained clip model is used in generating text embedding.",
      "start_time": 3130.16,
      "end_time": 3135.44
    },
    {
      "text": " Why is clip used here?",
      "start_time": 3135.44,
      "end_time": 3138.08
    },
    {
      "text": " Because our job is to generate image.",
      "start_time": 3138.08,
      "end_time": 3140.12
    },
    {
      "text": " I want the text embedding close to image embedding.",
      "start_time": 3140.12,
      "end_time": 3142.68
    },
    {
      "text": " So, clip is used.",
      "start_time": 3142.68,
      "end_time": 3144.0
    },
    {
      "text": "",
      "start_time": 3144.0,
      "end_time": 3120.0
    },
    {
      "text": " So second step in the stable diffusion is to send a text from",
      "start_time": 3127.0,
      "end_time": null
    },
    {
      "text": " to clip and generate text embeddings. This is the second step. Now, the third step is",
      "start_time": 3150.0,
      "end_time": 3172.36
    },
    {
      "text": "",
      "start_time": 3172.36,
      "end_time": 3150.0
    },
    {
      "text": " third step text embedding and the latent vector.",
      "start_time": 3155.72,
      "end_time": null
    },
    {
      "text": " is produced given to unit. The latent vector and the text embedding both are given to the",
      "start_time": 3180.0,
      "end_time": 3193.12
    },
    {
      "text": " unit architecture, both are inputs and what is the job of the unit? It produces noise",
      "start_time": 3193.12,
      "end_time": 3204.0
    },
    {
      "text": "",
      "start_time": 3204.0,
      "end_time": 3180.0
    },
    {
      "text": " prediction.",
      "start_time": 3180.6,
      "end_time": null
    },
    {
      "text": " That is the third step in stable diffusion.",
      "start_time": 3210.0,
      "end_time": 3220.44
    },
    {
      "text": " Unit architecture receives two inputs latent vector with noise and then text embedding and",
      "start_time": 3220.44,
      "end_time": 3232.92
    },
    {
      "text": "",
      "start_time": 3232.92,
      "end_time": 3210.0
    },
    {
      "text": " produces noise predictions.",
      "start_time": 3216.52,
      "end_time": null
    },
    {
      "text": " How is the text embedding going to help? Suppose text embedding is that of a car, text embedding",
      "start_time": 3240.0,
      "end_time": 3249.36
    },
    {
      "text": " is that of a cat. So the amount of noise that you have to remove, so if it is a car latent",
      "start_time": 3249.36,
      "end_time": 3256.92
    },
    {
      "text": " vector, I am writing the image itself, it is in car I do not know how to write, some",
      "start_time": 3256.92,
      "end_time": 3262.16
    },
    {
      "text": " car I am writing. This is the car, my cat and car both look similar, this is the cat.",
      "start_time": 3262.16,
      "end_time": 3269.84
    },
    {
      "text": "",
      "start_time": 3269.84,
      "end_time": 3240.0
    },
    {
      "text": " Thank you.",
      "start_time": 3241.0,
      "end_time": null
    },
    {
      "text": " This is the car horribly looking like car and not good at writing 3D diagram is looking",
      "start_time": 3270.0,
      "end_time": 3278.68
    },
    {
      "text": " like a house some wheels I will put maybe it will look like a car this is a car this",
      "start_time": 3278.68,
      "end_time": 3288.08
    },
    {
      "text": " is a cat okay I am not showing it in latent vector space I am showing it in image space",
      "start_time": 3288.08,
      "end_time": 3294.44
    },
    {
      "text": "",
      "start_time": 3294.44,
      "end_time": 3270.0
    },
    {
      "text": " So, the way I am adding noise here, I have added noise here, I have added noise here,",
      "start_time": 3277.0,
      "end_time": null
    }
  ],
  "transcript_text": " .  .  .  .  .  .  .  .  .  .  .  Thank you.  I'm sorry.  The   you  The  Good morning. Shall I start? In fact, last three days I was in CEDAC, which is a government  organization and I got to hear a lot of technology horizons and one day was on AI and LLM and   I felt very happy that we are on par with what they are working.  So, I did in fact, it is not about talking about myself, I just want to say when there  were some questions asked by C. Rack, I was able to answer, when the speaker did not answer,  but I answered it.  So, that happened, so that gave us lot of satisfaction and in fact, they all came back  to me, reached out, said that they want to collaborate and all that because we started  answering these questions.   So that was the success and they want to come back and conduct hackathons and  provide internships and all of this. So I have had some success in meeting them.  Hopefully we will get to know more about it and if I can facilitate some internships  it would be good. So this is the success but apart from this what I felt is we are doing  everything in silos where we lack here is that we teach LLM separately, high performance  clusters separately. There is no connectivity between the bottom layer like the Q-Lora thing   half precision full precision and all of this okay we do not connect so much.  So all the topics are conducted in silos, so we really do not get to know the entire  stack of layers in a single project or a single course but that is anyway up to you to connect  all the different pieces.  So lot of very good work is happening and there is lot of emphasis given on supercomputers  and developing processes, ohm processor is what they are developing, a processor from   India and they want to develop LLM from India also like this on India.  We are already developing one on Sanskrit.  So that work was happening on mental health they are working on.  So we are working on that project as well.  So I mean there is whatever projects we have given here.  In fact similar work is happening over them.  So that gives us lot of satisfaction and peace that we are doing correct here.  So that is what happened.  But only thing is they have access to very high end computing and there is funding there   no issue at all for them.  So and the data set they are in collaboration with Neman's.  So Neman's will provide a lot of data and they are in collaboration with the India's  all the repositories.  So digitization everything they are doing it but right now one pain point is that they  cannot share those resources to us.  So if in the absence of that how do we get hold of that so that is what we were discussing  so let us see how we proceed.  So this is in gist of whatever happened in the last three days and quantum computing   is also going in.  a big way that is what I saw that lot of emphasis is given, semiconductor design there is lot  of emphasis given.  So I am not sure in which field you would like to and one of the IAC person also spoke  about some neural computing related, neural I do not remember the exact name.  So there is one he actually did it damn good okay from last 10 years and that is the way  to compute he says.   So there are different computing dimensions we need to look at developing LLM on.  So, that marriage of computing and LLM that we have not discussed.  So, that is the gap that we have in the classes.  So, this is my understanding I thought I will just share some information with you on that.  So, with that now we will start.  So, so far we have looked at in the course in the last 3 units we have looked at how   we are able to generate the text in a better way by incorporating techniques like prompt  Thank you.  engineering, NENRAC techniques and all of that, so that the response from the LLM would  be more.  But primarily our focus was on text generation rather than any other multimodal applications.  So from today onwards, this is the fourth unit.  So our emphasis is on multimodal plus there is another important concept in LLM that you  need to understand which is the agentic approach.  So these are the two main highlights that we need to talk about.  So today we will discuss on that.   time  to first speak about something called as a image generation. Image generation although  is not exactly an LLM like text in it is similar in LLMs in the sense that LLMs are good at  generating text and there are generation engines which are good at images. In that sense image  generation can also be called similar to that of LLM but it is not exactly LLM because  it is not a large language kind of model.   So, those are the techniques that we need to look at.  and then we will proceed to multimodal LLM. So, today's main agenda is about image generation.  So, I think in your other TDL applications you must have already seen this concept. Have   you looked into any of the algorithms which incorporates image generation in the  topics in deep learning course, GAN, VAE, GAN and so on right. So, these are the techniques  that have been adopted in deep learning to generate images. I will speak about VAE little  later, first let us look at how the GAN operates, Generative Adversarial Network. The GAN is   do that generate.  So, how does it create these images?  What are the two main pieces of GAN if you are aware of it?  It has a discriminator and a generator.  So, there is a discriminator and there is a generator and to the discriminator you are  going to give from real images, real images and then generator is going to generate fake   images and discriminators job.  job is to distinguish that the image given by the generator is fake, it has to discriminate  between these two. So, the training of the discriminator is done keeping the generator  constant, generator is fixed during training time generator is fixed it generates fake  image and this fake image distribution should be as close to the distribution of the real  image. So, that is the job of the generator, the discriminator job is to distinguish between   the two. So, they do.  So, in like devil's advocate kind of scenario, so first discriminator is trained to make  sure that it discriminates, but generators job is then discriminator switch and then  generator is trained. Generator job is to produce an image which is as close to real  as possible that is the objective of the generator function.  Now, so these basically we are comparing the distributions of the images produced by the   the generator and the, so, the generator is the generator.  The real image and the discriminator's output is compared, the distributions are compared  and when we compare the distributions, how are the two distributions compared?  We either use KL divergence, full back libner divergence function for comparing the two  distributions or Jensen-Shanen divergence or Wasserstein-Gern you must have heard about.  So Wasserstein's distance is also used.   So these are the methodologies, they are different from your regular laws.  function. If you want to compare distribution versus distribution, one distribution is like  this, another distribution is like this. If you want to compare these two distributions,  this is fake image, this is real image. If you want to compare these two distributions,  you have these specialized loss functions called as Kulbach-Libner, Jensen's Shannon  divergence and versus strain distance. They compare distributions. So the overall area   is compared it is split and all of that. So, from internal operation.  are done that is not the topic of discussion now in detail but you will have to study this.  So these losses are going to be reduced in the case of VAE autoencoder, GAN and all of  this okay. So just to see that whether the distribution is closed or not. So like that  the generator produces but how does the generator produce this image? How does it generate the  image? It generates that image from a noisy image which is from a Gaussian distribution.   samples for Gaussian.  distribution and that is a sampled one is a noisy image. So, you are going to sample  a noisy image and you are going to produce that of a cat or something man face of a person.  So, the generator's job here in the GAN case the generator's job is very drastic in the  sense from one noise image directly you will produce the image that is the job of the generator.   This is again a CNN, this is again a CNN. Both are deep networks.  a deep network, this is a deep network and from this noisy image you are going to produce  this image, it is just one step process, noise converted into an image, but the drift should  be towards real image. If real image consists of all face images and this produces all car  images, then it is there is a total drift in the probability distribution. So, then this  divergence is too much, so the divergence you try to minimize. So, that is how the KL   divergence is going to be used in.  the GaNS equations there will load whole lot of mathematics behind that but anyway if you  understand the idea behind what GaNS does it is good enough for our discussion now.  So the generator the way the GaNS operate in creating a image is very drastic from very  noisy image generally you expected to produce a distribution which is as close to real okay  very drastic step. So that in that sense the GaNS were not really very good at producing   high quality images.  images. GANS suffers from producing high quality images. So this was the issue with the GANS.  So that is when they thought of using a different methodology altogether to generate or produce  high quality images. So these are called as diffusion models which is used in stable diffusion.   So that is the topic of our discussion now.  We will have to study diffusion models.  You have to concentrate on the class, close the laptop, diffusion models.   So, in the diffusion models, unlike GANS, we do a step by step process.  There are two processes followed, one is a forward process and the second one is a reverse  process.  Forward process is used for some training, reverse process is actually used for generation.  Forward process is used in training the stable diffusion models and reverse process is used   at the time of.  inferencing the sense here the objective is to generate images. So what the forward process  does is you are given an image training image. So training must have proper images. So training  set is consisting of proper images of that of a person or car, cat, dog whatever. So  this is a proper image. The in the forward process what you are going to do is to add   some noise add some noise  into this. So and that noise is going to be scheduled. How much noise you have to add  is characterized by a term called beta t, beta t. Beta t talks about the noise schedule,  how much to be added at time stamp t that is called as beta t, noise control parameter.  So beta t will get added to this and you will be producing another image but with some noise   added some nice added  then this is a time step this is a t 0 let us say this is a time step t 1 again at time   timestamp t2, further noise is added already this is getting blurred ok. So, like this  if you are  Yeah, beta is derived from the sample from a normal distribution. Yeah, every time the  noise has to be sampled from a normal distribution and the noise is injected into the image.  It is not the same amount, every time it is a different amount of noise which is drawn   from a normal distribution.  like this, like that. This is the forward diffusion process, ok.   the problem.  probability of generating x t from x t minus 1 probability see this is a probability what  kind of image you will generate from this is a probabilistic number because you are  inducing any sample can be drawn. So, it is a probability probability of generating x  t given x t minus 1 is equal to from a normal distribution you are going to generate x t   can call it as x t, x t.  1 minus 1 minus beta t root xt minus 1 beta t i. This is the mean, this is the variance.  This is drawn from a normal distribution with this mean and this variance you are going   to generate xt that is the meaning of this equation forward diffusion process probability of the diffusion process.  of generating x t from x t minus 1 is based on deriving the sample from a normal distribution  with mean mu and sigma variance and deriving the image x t that is the meaning of this  equation this is the forward diffusion equation. So, every time you are going to draw the noise  set the beta parameter and draw the noise from a normal distribution and then you are   going to inject this noise at successive samples. So, this is known as a noise schedule.  The noise schedule has been set to T's time stamps, T plus 1 basically, but T 0 is not  a noisy image, but rest all the images are noisy images. T 1 to x t, okay, T represents  the time stamp and x represents the image, x t minus 1 is the image that is produced  at time stamp T minus 1 and this is produced at time t. Finally, a complete noisy image   produced.  Exactly the reverse process is in reverse diffusion process.  So, you are going to start from Xd a noisy image that is what the GAN did.  The GAN's approach is like this, you have a noisy image Xd and GAN what it did do?  It directly produced with this image.  The GAN produced that noisy image directly into this that is where the stable diffusion  differs or the diffusion models differ.   Instead of producing from that noise,  see image to this image which is very drastic. So, it is really difficult to learn such a  process what it does is it produces this image first. So, in the reverse diffusion process   q of x t minus 1 is generated from x t is again using another normal distribution.  So, this is the reverse distribution process. It is a probability of generating this image,  given this image, Q is the probability of generating Xt minus 1 given Xt from a normal  distribution again, you generate Xt minus 1 taking mu and sigma and then you are going   to formulate that.  is basically the process but how is it done in implementation that we will see later but  this is what it does okay.  So once the noise is characterized in the reverse diffusion process you are going to  subtract that noise and you are going to get this and subtract the noise and get it and  so on.  So normal distribution is going to fetch the noise for you okay and that noise has to be  subtracted and you are going to get a better image and it will drift towards the kind of   the image that you want.  So, in successive steps, so given xt you are not going to generate x0 directly, given xt  you are going to generate xt-1, given xt-1 you are going to generate xt-2, xt-2 will  produce xt-3.  So, small amount of noise is reduced, okay, how much noise to be reduced from a given  image that has to be seen, but it does that.  In the reverse process basically it is about subtracting the noise in the image so that   you get to that ship.  But how much noise and where that noise has to be reduced has to be learnt.  It is not like simply removing noise.  You have to get to drift it towards a person.  You have to drift it towards a car.  You have to get that image right.  So how is that drifting done is what is done in the stable diffusion.  But generally diffusion models work based on this principle of adding noise in the forward  process and removing noise in the reverse process.   diffusion process as successfully noise to successive images.  and moves towards pure noise image and in the reverse diffusion process from the pure  noise you start removing noise at random places and drift towards generating an image which  you want. So that is the reverse diffusion process. Both of them are drawn from a normal  distribution the noise related aspects. So this is forward and reverse diffusion.   Now if you look at this image let us say this is some 100 percent.  500 cross 100 or 500 cross 500 resolution image. So this 500 cross 500 you have to do  it for T times times. So they felt that computationally this is very very expensive adding noise  every time to a 500 cross 500 pixel which is like 25, 250,000. 250,000 set of values   will have to be perturbed.  will have to be perturbed every time by adding noise. So, 250,000 here, 250,000 here, 250,000  here, it is still a very small resolution. So, high resolution 2000 and all that will  be very, very expensive. So, what they decided is instead of the diffusion  model they went ahead something with a concept called as latent diffusion model. So, instead   of operating in the image space.  the diffusion models operate in the image space whereas the latent diffusion models  operate in the latent space. So what do we mean by latent space? Do you have an idea  about it? What is the latent space? That is nothing but the feature vector right, feature   vector of the image.  m cross n resolution image converted into a latent vector which is very small in size.  This is the latent vector. Suppose you have a 500 cross 500 into 3 channels RGB channels,   So you can convert it into  let us say 4 x 64 x 64 this is still a very small vector space. This is 500 x 500 x 3  using an encoder convert it into a latent vector. For example, I have the image of a  face. The image of a face converting to latent vector means one dimension of the latent vector   can capture skin tone from the face you do not need all pixels of the face.  I just need the skin tone so that can be one or two dimensions representing the skin tone.  If there are too many skin tones possible then one bit may not be sufficient.  So you may need the multiple bits to represent skin tone so that is this and then you want  to talk about the hair color so that is another type and then you want to speak about the  expression so that is another like this you are going to capture the feature vectors from   the face that is what is the encoder trying to do.  feature maps we say right in conventional neural network it generates the feature map.  The feature map is nothing but a latent vector, it produces a vector which consists of your  facial pictures or the features and that can be produced as a distribution, the skin distribution,  the hair color distribution, it can generate distribution of the various people in terms  of the skin, in terms of the hair and all of that various distribution can be produced   or directly you see.  for this phase this is the latent vector direct okay. A latent vector representation for phase  1, latent vector representation for phase 2 means that it is a feature vector. As an  example for phase the feature vector for phase consists of capturing skin tone, hair color,  smile, pose and so many things, eyes whatever you want to characterize as. So depending on  that you can define the latent vector space and basically it is a low dimensional vector   drawn from the high dimensional.  image that is what we mean by latent vector is that clear in all of the deep learning  techniques we will be operating with latent vectors. So, this is what is going to happen  and then given the latent vector you can produce the image given this latent vector you can  produce the image again this is the decoder ok.   the image you call out the features into a latent vector and this latent vector is the  vector produced given to decoder will actually reproduce that image back. So, this is called  as an identity function, it produces an image back. So, this function is called as an identity  function. What type of architecture is this? It is an autoencoder, this is an autoencoder,   autoencoder. Autoencoder's job is to produce an identity function.  Is this clear? Now, I was talking about in the diffusion models, the forward and reverse  diffusion process. If the forward and reverse diffusion process applies noise on the images,  the latent diffusion models apply noise on the latent vector instead of the image space,  that is the difference. You understand? Instead of applying noise on this image, we apply noise   from the latent vector which is a low dimensional representation of the given vector.  than input that is it as simple as that. So, that is called as a latent diffusion model.  If you operate in the latent space it is called as latent diffusion model. If you operate  in the image space it is just called as a diffusion model. This is clear is the difference  clear. Now, how are you going to generate this latent vector you can use any encoder decoder  model one of the examples is auto encoder ok. And the VAE is different how is auto encoder   different from that of the VAE.  You must have studied in deep learning right or this is not there, is this there or not  there, it is there, tell me.  VIE, the difference between autoencoder and variational autoencoder is that it directly  from the given image directly a latent vector if it is produced, it is called as autoencoder.   it produces only the mu and sigma distributed.  distribution if it produces skin color distribution for set of faces. I told you about the hair  color distribution and then eyes color distribution. So if it produces different distribution instead  of directly the vector, instead of directly the vector if it produces a distribution and  from this distribution you produce a latent vector then it is called as variational auto   encoder and in variational order encoder there is some reverse back  propagation step but it breaks here because from the distribution you are generating you  are sampling the latent vector.  Directly you will not produce a latent vector I hope you are getting what I am saying you  are generating the output of the encoder if you do not understand too much about it you  just understand this that in the auto encoder directly low dimensional vector is produced  for the image.  In the case of variational auto encoder it produces a distribution values from which you   you will sample and produce a latent vector.  You will sample from this distribution and produce a latent vector corresponding to the  image.  So, directly the latent vector is not given.  So, there will be some gap in this operation.  So, there is something called reparameterization trick and all that is used in VAE which is  a very important concept that you should know.  So, when you are studying you look at that reparameterization trick and also that KL  divergence and then lot of operations here to be performed to make the VAE very powerful.   So that's it.  that you need to study if you are studying VAE but the reason why I am talking about  VAE is this is one of the first steps in stable diffusion it uses VAE.  So, I was under the assumption that you will know about it so I did not want to discuss  this in too much detail of the math involved in it but you should know about it.  It is a very important aspect, reparameterization trick you should know in VAE and also the  KL divergence that you should know.  So, this is how the latent vector is produced.   So, for our discussion in the LLM class, we will be discussing the LLM class.  we just need to know that the input image produces a distribution from which a latent  vector is produced and this latent vector is further used in the forward and reverse  processes for generating images.  Is this clear?  So far is this clear?  What is the diffusion model?  What is the forward process?  What is the reverse process?  What is the characteristic equation that defines the generation of the next image, the probability   function and what is the characteristic equation that defines the reverse function.  process. These aspects should be clear and we do not operate in the image space, we operate  in the latent space because the image space is very big and too many noise schedules,  the computation cost is very very high and therefore, we only operate in the latent  space and then at the last step only we convert to image space.  The starting step is image space, the last step is image space and all the in between  forward and reverse steps are carried out only in the latent space, is this clear? That   That is why this model is called as.  latent diffusion model which is used in stable diffusion. This is clear. Now, we will proceed  to stable diffusion. This is the theory that was needed. Stable diffusion, stable diffusion   is a architectural design for generating images.  This is a recent, recent almost 5 years back I think it is introduced, Dali and all of  that they introduced first image generation. They are all based on diffusion models. This  is not a transformer model. This does not use transformer, it uses diffusion models  to generate images. This is different in that sense. So, its first step is to generate the   latent vector.  The first step in latent stable diffusion for generating images is to produce a latent  vector and to produce a latent vector it uses VAE, variational autoencoder. It uses variational  autoencoder to generate a latent vector for any given image. You have to train the stable  diffusion model. To train the stable diffusion model, first of all you need for it to generate   be made  at a later stage you need to train the model on set of images first okay that training  process that I am talking about you are given a set of images and these images latent vector  representations you need to produce using the diffusion I mean using the VAE variational  auto encoder models okay.  Then the next step is to add the noise and figure out how much noise has been added so   So add noise.  That is the forward process, add noise and predict how much noise is added.   I will call this as third step in between sometimes there is conditioning done, conditioning  Thank you. Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  using text prompt and for this it uses contrastive learning image pre-training clip and this  uses a unit architecture.   The second step is conditioning using text prompt. Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  Contrasting.  learning image pre-training architecture is used, I will explain that. These are the three  steps in stable diffusion training. This is a training process.   is a encoder decoder architecture there is no expansion  for that, it is a U architecture, U type of architecture, U convolutions and deconvolution,  decoder that is called a Unet. If there is any name for U, I am not sure about that.  It is in a form of U. It is also like a transformer, clip is also a transformer. VAE is CNM,   is a CNN.  unit also is cnn but it is like transformer. These are the three steps. So what is the  first step? VAA just now I wrote the picture. So you take an image and produce the latent   So, it is a latent vector encoder.  You can use a pre-trained VIE, pre-trained VIE so that it is able to generate the image,  it is an identity function I told you.  What does that mean?  If image X1 is given, it produces X1, X1 prime which is as close to X1.  So X1 will be approximately equal to X1 dash, it is an identity function.   So again,  This is the encoder, this is the encoder, this is the decoder, decoder, this is the VAE architecture.  The encoder convolution layer produces from the given image latent vector representation,  from the latent vector representation you give it to decoder produces x1 dash.  This is the first step, we are using a pre-tent VAE, so it automatically produces the latent   How do you know that?  that the latent vector is correct because it produces back x1 dash which is as close  to the original distribution. So, it is the correct representation of the image. There  is not much of loss in the information here. So, we are going to pick up this latent vector.  This is the first step that is this latent vector. So, what is the next step? See in  the next step, we have to condition based on the prompt.   So, we are going to say that I want to say that I want to say that I want to say that  want to this is a cat image so it is a pair the image is that of a cat I do not know how  to write the cat and all that so this is cat okay so this is a cat image this is a text   This is a text prompt.  It goes through clip, contrastive, learning, image, retraining.   is the transform on model used.  I will slightly digress now, let us see how clip is trained on.  In stable diffusion we are using a pre-trained clip, but how is the training done for clip  itself we will see now.  The clip uses two architectures, clip uses two architectures.  I am now talking about multimodal LLM, slightly degress from image generation to multimodal  LLM.   Clip is an example of a multi-modal LLM.  What is a multimodal LLM?  It access other than text input other types of data like in this example image, image  it access, text it access, generates text, generates text embedding that is what we mean  by multimodal LLM.   It takes different...  types of inputs in generating of embeddings that is called as a multimodal LLM.  So here clip accepts image text pairs, image text pairs.  See there is slight deviation from the original idea that is it but it has become very popular.  You must have heard about clip blip and all of that right but it became popular.   What did they do? I will tell you now, just now.  Just in a minute.  So here they have used a vision transformer.  They have used a vision transformer.  What is the vision transformer we have studied already?  What is it?  Given an image it fits it into patches.  Transformer it expects sequence.  This you should remember forever.  Transformer means it is a sequence to sequence model.  We should give some sequence to transformer.  To give a sequence image is not a sequence basically.   How do you convert?  an image into a sequence, piece it, scissor it and then make the sequence and produce  that sequence. So, image sequence is given, image sequence is given and it produces image  embeddings, image embeddings. This is one architecture used in clip. On the other side   it uses GPT kind of architecture decoder, GPT type of decoder.  and text prompt is given and it produces text embeddings.  Text embeddings are produced, two architectures separately in the clip model and these image  embeddings and text embeddings let us say we are giving the image of a cat, image of  a cat and the text is also saying cat image, text is also saying cat image.   So the embedding generated by this is the same as the previous one.  these two should be close, they have to match right, it is a match pair, image and text  match pair. So if it is not matching it will retrain saying that these embedding should  come closer to each other. So checks the cosine similarity between the two that is why it  is called as contrastive learning, cosine similarity between the text embedding and  image embedding and if it is not close to one there is lot of error because we know   that we have given matching.  we gave and the text is also cat image. The two embeddings should be closed. If the two  embeddings are not closed then there is high loss. High loss will back propagate and train  so that the loss between the two embeddings is reduced. What does that mean? It learns  the equivalent text embedding and image embedding it brings them to same space. That is what  the clip is trying to learn. Suppose I give that this is a dog image is a text prompt   But what I am giving...  is a cat and now you check whether the embedding are really dissimilar or not is there actually  very similar then there is again there is a loss. So you train so that they become far  apart so for similarity you can keep it one for dissimilarity you can keep it zero any  output you can keep. So if they are not close to this if they are similar they have to be  close to one if they are not close to one there is a loss if they are dissimilar information   they have to be close to zero if they are not close to zero.  to 0 again there is a loss. So, like this they gave similar pairs dissimilar pairs like  this a lot of may be 1 k 2 k 2 lakhs I am saying 2 lakh images and then they trained  this transformer this is how clip is trained. Now, once the clip is trained now you can give  that just the text and the text embedding generated would be in the space similar to  that of the image. You understand the difference between the   birth embedding and this also text embedding it can do.  So, once the clip is ready you can just give text prompt and you can get the text embeddings.  What is the difference between clips text embedding and birds bird same text what is  the difference the embedded same sentence let us say cat image cat image to bird cat  image to clip if you give the embedding generated by the clip will be similar to image embedding   Thanks, please.  The two embeddings will not be same.  The training tasks are different.  Do you understand?  The birds has been trained only on missing word prediction.  The way the embeddings are generated by birds is totally different.  For the same sentence, do you understand the difference and can you appreciate?  It is very important on what they have been trained on, what is the task at which they  are good at and based on that only they will produce the embedding.   Thank you.  Although the sentences are same, the kind of embedding produced by BERT would be very  different from the embedding produced by CLIP and the CLIP embedding would be close to image  type of embedding.  So you should know if you are operating in image related applications, the text prompt  embedding should be close to image related embedding.  You cannot use a BERT there.  You understand when you give a text prompt and you want it to be converted into an embedding   and you are using that for generating an image.  Don't you think we should use clip now?  Do you understand what I am saying?  So the kind of prompt you gave cat image, cat image, here also cat image but this produce  0.1, 0.2, this may produce 0.6, 0.5.  The two embeddings are different because the training that has been given is different.  You get what I am saying.   So 0.6, 0.5 will be similar to that of image embedding which is closed.  And if you are talking about image applications you should use such pre-trained models.  Now you understand hopefully which pre-trained model you should choose.  It is not just like that some pre-trained model.  Your first understanding should be what is it trained on?  Is it going to serve my purpose or not?  Do you understand?  Lot of pre-trained models are available.  Just like that you cannot pick up something.   I use the VGG, I use the ResNet, like that you use.  It is not generic, you have to decide what task it is trained on whether it is going  to suit my application or not.  Then it will be closer not that the bird will not work, but those embeddings will be far  away from your image embeddings.  They may not really serve the purpose so much.  So, that is the importance of which pre-trained model.  Pre-training means that you should know the task on which it is pre-trained.   So that you know what kind of embeddings get generated from the...  pre-trained model and whether those embeddings will make sense in my application or not and  based on that you should proceed further.  Is this clear?  Okay.  So, is clip clear?  So, the pre-trained clip model is used in generating text embedding.  Why is clip used here?  Because our job is to generate image.  I want the text embedding close to image embedding.  So, clip is used.   So second step in the stable diffusion is to send a text from  to clip and generate text embeddings. This is the second step. Now, the third step is   third step text embedding and the latent vector.  is produced given to unit. The latent vector and the text embedding both are given to the  unit architecture, both are inputs and what is the job of the unit? It produces noise   prediction.  That is the third step in stable diffusion.  Unit architecture receives two inputs latent vector with noise and then text embedding and   produces noise predictions.  How is the text embedding going to help? Suppose text embedding is that of a car, text embedding  is that of a cat. So the amount of noise that you have to remove, so if it is a car latent  vector, I am writing the image itself, it is in car I do not know how to write, some  car I am writing. This is the car, my cat and car both look similar, this is the cat.   Thank you.  This is the car horribly looking like car and not good at writing 3D diagram is looking  like a house some wheels I will put maybe it will look like a car this is a car this  is a cat okay I am not showing it in latent vector space I am showing it in image space   So, the way I am adding noise here, I have added noise here, I have added noise here,"
}